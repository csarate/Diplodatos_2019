{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\"\"\"Exercise 1\n",
    "\n",
    "Usage:\n",
    "\n",
    "$ CUDA_VISIBLE_DEVICES=2 python practico_1_train_petfinder.py --dataset_dir ../ --epochs 30 --dropout 0.1 0.1 --hidden_layer_sizes 200 100\n",
    "\n",
    "To know which GPU to use, you can check it with the command\n",
    "\n",
    "$ nvidia-smi\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### PrÃ¡ctico 1 Aprendizaje Profundo de: Claudio Sarate - Martin Hunziker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import mlflow\n",
    "import numpy\n",
    "import pandas\n",
    "import easydict\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from itertools import zip_longest\n",
    "from collections import Iterable, Iterator\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "in_colab = 'google.colab' in sys.modules\n",
    "if in_colab:\n",
    "    !pip install tensorflow==2.0.0\n",
    "    !pip install mlflow\n",
    "    print(tf.__version__)\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    BASE_DIR = \"/content/drive/My Drive/Colab Notebooks/Machine Learning/Aprendizaje Profundo/Data/\"\n",
    "else:\n",
    "    BASE_DIR = \"./Data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "TARGET_COL = 'AdoptionSpeed'\n",
    "embed_size = 5"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "os.environ.keys()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "'jupyter_notebook' in sys.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def read_args():\n",
    "   \n",
    "    #parser = argparse.ArgumentParser(\n",
    "\n",
    "#    description='Training a MLP on the petfinder dataset')\n",
    "    # Here you have some examples of classifier parameters. You can add\n",
    "    # more arguments or change these if you need to.\n",
    "#    parser.add_argument('--dataset_dir', default=BASE_DIR, type=str,\n",
    "#                        help='Directory with the training and test files.')\n",
    "#    parser.add_argument('--hidden_layer_sizes', nargs='+', default=[100], type=int,\n",
    "#                        help='Number of hidden units of each hidden layer.')\n",
    "#    parser.add_argument('--epochs', default=10, type=int,\n",
    "#                        help='Number of epochs to train.')\n",
    "#    parser.add_argument('--dropout', nargs='+', default=[0.5], type=float,\n",
    "#                        help='Dropout ratio for every layer.')\n",
    "#    parser.add_argument('--batch_size', type=int, default=64,\n",
    "#                        help='Number of instances in each batch.')\n",
    "#    parser.add_argument('--one_hot_cols', type=str, nargs='+', default=['Gender', 'Color1'],\n",
    "#                        help='One hot columns.')\n",
    "#    parser.add_argument('--numeric_cols', type=str, nargs='+', default=['Age', 'Fee'],\n",
    "#                        help='Numeric columns.')\n",
    "#    parser.add_argument('--embedded_cols', type=str, nargs='+', default=['Breed1'],\n",
    "#                        help='Embedded columns.')\n",
    "#    parser.add_argument('--experiment_name', type=str, default='Base model',\n",
    "#                        help='Name of the experiment, used in mlflow.')\n",
    "#    parser.add_argument('--run_name', type=str, default='run00',\n",
    "#                        help='Name of the run, used in mlflow.')\n",
    "#    args = parser.parse_args()\n",
    "\n",
    "#    assert len(args.hidden_layer_sizes) == len(args.dropout)\n",
    "#    return args\n",
    "\n",
    "    #Cambio para correr en jupyter notebook con argumentos que se definen dentro\n",
    "    args = easydict.EasyDict({'dataset_dir': BASE_DIR,\n",
    "                              'hidden_layer_sizes': [10, 100],\n",
    "                              'epochs': 100,\n",
    "                              'dropout': [0.3, 0.5],\n",
    "                              'batch_size': 64,\n",
    "                              'one_hot_cols': ['Gender', 'Color1'],\n",
    "                              'numeric_cols': ['Age', 'Fee'],\n",
    "                              'embedded_cols': ['Breed1'],\n",
    "                              'experiment_name': 'Base Model',\n",
    "                              'run_name': 'run01'})\n",
    "\n",
    "    assert len(args.hidden_layer_sizes) == len(args.dropout)\n",
    "    print (args)\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def process_features(df, one_hot_columns, numeric_columns, embedded_columns, test=False):\n",
    "    direct_features = []\n",
    "\n",
    "    # Create one hot encodings\n",
    "    for one_hot_col, max_value in one_hot_columns.items():\n",
    "        direct_features.append(tf.keras.utils.to_categorical(df[one_hot_col] - 1, max_value))\n",
    "\n",
    "    # TODO Create and append numeric columns\n",
    "    for column in numeric_columns:\n",
    "        direct_features.append(tf.keras.utils.normalize(df[column].values.reshape(-1,1)))\n",
    "\n",
    "#    Otras Alternativas\n",
    "\n",
    "#        scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "#        scaler.fit(df[column].values.reshape(-1, 1))\n",
    "#        df[column] = scaler.transform(df[column].values.reshape(-1,1)).reshape(-1)\n",
    "#    for col in df[numeric_columns]:\n",
    "#        direct_features.append(df[numeric_columns].values)   \n",
    "\n",
    "#    Otra alternativa\n",
    "#        scaler_features = preprocessing.StandardScaler().fit_transform(df[[num_column]].values)\n",
    "#        direct_features.append(scaler_features)\n",
    "\n",
    "    # Concatenate all features that don't need further embedding into a single matrix.\n",
    "  \n",
    "    features = {'direct_features': numpy.hstack(direct_features)}\n",
    "\n",
    "    # Create embedding columns - nothing to do here. We will use the zero embedding for OOV\n",
    "    for embedded_col in embedded_columns.keys():\n",
    "        features[embedded_col] = df[embedded_col].values\n",
    "\n",
    "    if not test:\n",
    "        nlabels = df[target_col].unique().shape[0]\n",
    "        # Convert labels to one-hot encodings\n",
    "        targets = tf.keras.utils.to_categorical(df[target_col], nlabels)\n",
    "    else:\n",
    "        targets = None\n",
    "    \n",
    "    return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def load_dataset(dataset_dir, batch_size):\n",
    "\n",
    "    # Read train dataset (and maybe dev, if you need to...)\n",
    "    dataset, dev_dataset = train_test_split(\n",
    "        pandas.read_csv(os.path.join(dataset_dir, 'train.csv')), test_size=0.2)\n",
    "    \n",
    "    test_dataset = pandas.read_csv(os.path.join(dataset_dir, 'test.csv'))\n",
    "    \n",
    "    print('Training samples {}, test_samples {}'.format(\n",
    "        dataset.shape[0], test_dataset.shape[0]))\n",
    "    \n",
    "    return dataset, dev_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def define_model(embedded_input, direct_input, hidden_layers, dropouts, nlabels):\n",
    "    tf.keras.backend.clear_session()\n",
    "    initializer = tf.keras.initializers.glorot_normal(seed=1234)\n",
    "\n",
    "\n",
    "    # Add one input and one embedding for each embedded column\n",
    "    embedding_layers = []\n",
    "    inputs           = []\n",
    "    for embedded_col, max_value in embedded_input.items():\n",
    "        input_layer = layers.Input(shape=(1,), name=embedded_col)\n",
    "        inputs.append(input_layer)\n",
    "        # Define the embedding layer\n",
    "        embedding_size = int(max_value / embed_size)\n",
    "        embedding_layers.append(\n",
    "            tf.squeeze(layers.Embedding(input_dim=max_value, output_dim=embedding_size)(input_layer), axis=-2))\n",
    "        print('Adding embedding of size {} for layer {}'.format(embedding_size, embedded_col))\n",
    "\n",
    "    # Add the direct features already calculated\n",
    "    direct_features_input_shape = direct_input.shape[1]\n",
    "    direct_features_input = layers.Input(shape=direct_features_input_shape, name='direct_features')\n",
    "    inputs.append(direct_features_input)\n",
    "\n",
    "    # Concatenate everything together\n",
    "    features = layers.concatenate(embedding_layers + [direct_features_input])\n",
    "    \n",
    "    # Creating Models\n",
    "    n_layers = len(hidden_layers)\n",
    "    if len(dropouts) > n_layers:\n",
    "        dropouts = dropouts[:n_layers]\n",
    "        \n",
    "    for n_neurons, drop, layer in zip_longest(hidden_layers, dropouts, range(n_layers)):\n",
    "        if layer == 0:\n",
    "            dense      = layers.Dense(n_neurons, activation='relu', kernel_initializer=initializer)(features)\n",
    "            last_layer = dense\n",
    "        else:\n",
    "            dense      = layers.Dense(n_neurons, activation='relu', kernel_initializer=initializer)(last_layer)\n",
    "            last_layer = dense\n",
    "        if drop is not None:\n",
    "            #drop_layer = layers.BatchNormalization()(last_layer)\n",
    "            #last_layer = drop_layer\n",
    "            drop_layer = layers.Dropout(drop)(last_layer)\n",
    "            last_layer = drop_layer\n",
    "    \n",
    "    output_layer = layers.Dense(nlabels, activation='softmax')(last_layer)\n",
    "    model        = models.Model(inputs=inputs, outputs=output_layer)            \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_dir': './Data/', 'hidden_layer_sizes': [10, 100], 'epochs': 100, 'dropout': [0.3, 0.5], 'batch_size': 64, 'one_hot_cols': ['Gender', 'Color1'], 'numeric_cols': ['Age', 'Fee'], 'embedded_cols': ['Breed1'], 'experiment_name': 'Base Model', 'run_name': 'run01'}\n",
      "Training samples 8465, test_samples 4411\n",
      "Adding embedding of size 61 for layer Breed1\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Breed1 (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 61)        18788       Breed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Squeeze (TensorFlow [(None, 61)]         0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "direct_features (InputLayer)    [(None, 12)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 73)           0           tf_op_layer_Squeeze[0][0]        \n",
      "                                                                 direct_features[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10)           740         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 10)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          1100        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 5)            505         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 21,133\n",
      "Trainable params: 21,133\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Running:  run01\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 6s 45ms/step - loss: 1.5177 - accuracy: 0.2539 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4712 - accuracy: 0.2796 - val_loss: 1.4497 - val_accuracy: 0.3094\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4566 - accuracy: 0.2887 - val_loss: 1.4454 - val_accuracy: 0.3231\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4495 - accuracy: 0.2975 - val_loss: 1.4434 - val_accuracy: 0.3330\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4471 - accuracy: 0.3011 - val_loss: 1.4446 - val_accuracy: 0.3321\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 1s 4ms/step - loss: 1.4437 - accuracy: 0.3082 - val_loss: 1.4441 - val_accuracy: 0.3269\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4422 - accuracy: 0.3157 - val_loss: 1.4443 - val_accuracy: 0.3255\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4375 - accuracy: 0.3121 - val_loss: 1.4458 - val_accuracy: 0.3217\n",
      "Epoch 9/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4349 - accuracy: 0.3220 - val_loss: 1.4456 - val_accuracy: 0.3198\n",
      "Epoch 10/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4340 - accuracy: 0.3275 - val_loss: 1.4479 - val_accuracy: 0.3212\n",
      "Epoch 11/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4328 - accuracy: 0.3226 - val_loss: 1.4473 - val_accuracy: 0.3217\n",
      "Epoch 12/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4280 - accuracy: 0.3264 - val_loss: 1.4498 - val_accuracy: 0.3212\n",
      "Epoch 13/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4296 - accuracy: 0.3289 - val_loss: 1.4502 - val_accuracy: 0.3226\n",
      "Epoch 14/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4265 - accuracy: 0.3305 - val_loss: 1.4518 - val_accuracy: 0.3198\n",
      "Epoch 15/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4274 - accuracy: 0.3226 - val_loss: 1.4538 - val_accuracy: 0.3193\n",
      "Epoch 16/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4233 - accuracy: 0.3342 - val_loss: 1.4538 - val_accuracy: 0.3203\n",
      "Epoch 17/100\n",
      "133/133 [==============================] - 1s 4ms/step - loss: 1.4215 - accuracy: 0.3281 - val_loss: 1.4559 - val_accuracy: 0.3217\n",
      "Epoch 18/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4210 - accuracy: 0.3278 - val_loss: 1.4561 - val_accuracy: 0.3240\n",
      "Epoch 19/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4186 - accuracy: 0.3338 - val_loss: 1.4570 - val_accuracy: 0.3198\n",
      "Epoch 20/100\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.4242 - accuracy: 0.3282 - val_loss: 1.4567 - val_accuracy: 0.3198\n",
      "Epoch 21/100\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.4189 - accuracy: 0.3299 - val_loss: 1.4583 - val_accuracy: 0.3203\n",
      "Epoch 22/100\n",
      "133/133 [==============================] - 1s 4ms/step - loss: 1.4205 - accuracy: 0.3341 - val_loss: 1.4586 - val_accuracy: 0.3193\n",
      "Epoch 23/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4187 - accuracy: 0.3334 - val_loss: 1.4582 - val_accuracy: 0.3174\n",
      "Epoch 24/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4168 - accuracy: 0.3320 - val_loss: 1.4618 - val_accuracy: 0.3174\n",
      "Epoch 25/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4171 - accuracy: 0.3308 - val_loss: 1.4638 - val_accuracy: 0.3193\n",
      "Epoch 26/100\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.4195 - accuracy: 0.3314 - val_loss: 1.4635 - val_accuracy: 0.3193\n",
      "Epoch 27/100\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.4111 - accuracy: 0.3340 - val_loss: 1.4630 - val_accuracy: 0.3203\n",
      "Epoch 28/100\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.4158 - accuracy: 0.3272 - val_loss: 1.4646 - val_accuracy: 0.3203\n",
      "Epoch 29/100\n",
      "133/133 [==============================] - 1s 5ms/step - loss: 1.4136 - accuracy: 0.3310 - val_loss: 1.4672 - val_accuracy: 0.3170\n",
      "Epoch 30/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4133 - accuracy: 0.3320 - val_loss: 1.4680 - val_accuracy: 0.3155\n",
      "Epoch 31/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4151 - accuracy: 0.3353 - val_loss: 1.4677 - val_accuracy: 0.3188\n",
      "Epoch 32/100\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.4137 - accuracy: 0.3314 - val_loss: 1.4674 - val_accuracy: 0.3188\n",
      "Epoch 33/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4159 - accuracy: 0.3297 - val_loss: 1.4650 - val_accuracy: 0.3179\n",
      "Epoch 34/100\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.4130 - accuracy: 0.3340 - val_loss: 1.4702 - val_accuracy: 0.3155\n",
      "Epoch 35/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4126 - accuracy: 0.3353 - val_loss: 1.4699 - val_accuracy: 0.3155\n",
      "Epoch 36/100\n",
      "133/133 [==============================] - 1s 4ms/step - loss: 1.4157 - accuracy: 0.3282 - val_loss: 1.4678 - val_accuracy: 0.3165\n",
      "Epoch 37/100\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.4142 - accuracy: 0.3341 - val_loss: 1.4673 - val_accuracy: 0.3118\n",
      "Epoch 38/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4137 - accuracy: 0.3312 - val_loss: 1.4664 - val_accuracy: 0.3174\n",
      "Epoch 39/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4108 - accuracy: 0.3348 - val_loss: 1.4690 - val_accuracy: 0.3179\n",
      "Epoch 40/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4098 - accuracy: 0.3388 - val_loss: 1.4701 - val_accuracy: 0.3151\n",
      "Epoch 41/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4109 - accuracy: 0.3373 - val_loss: 1.4706 - val_accuracy: 0.3151\n",
      "Epoch 42/100\n",
      "133/133 [==============================] - 1s 4ms/step - loss: 1.4124 - accuracy: 0.3356 - val_loss: 1.4712 - val_accuracy: 0.3132\n",
      "Epoch 43/100\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.4152 - accuracy: 0.3324 - val_loss: 1.4712 - val_accuracy: 0.3155\n",
      "Epoch 44/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4109 - accuracy: 0.3357 - val_loss: 1.4728 - val_accuracy: 0.3151\n",
      "Epoch 45/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4112 - accuracy: 0.3303 - val_loss: 1.4728 - val_accuracy: 0.3113\n",
      "Epoch 46/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4122 - accuracy: 0.3266 - val_loss: 1.4730 - val_accuracy: 0.3165\n",
      "Epoch 47/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4109 - accuracy: 0.3377 - val_loss: 1.4745 - val_accuracy: 0.3184\n",
      "Epoch 48/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4133 - accuracy: 0.3351 - val_loss: 1.4754 - val_accuracy: 0.3132\n",
      "Epoch 49/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4147 - accuracy: 0.3318 - val_loss: 1.4748 - val_accuracy: 0.3113\n",
      "Epoch 50/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4120 - accuracy: 0.3341 - val_loss: 1.4772 - val_accuracy: 0.3174\n",
      "Epoch 51/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4112 - accuracy: 0.3322 - val_loss: 1.4759 - val_accuracy: 0.3184\n",
      "Epoch 52/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4113 - accuracy: 0.3353 - val_loss: 1.4765 - val_accuracy: 0.3269\n",
      "Epoch 53/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4103 - accuracy: 0.3330 - val_loss: 1.4779 - val_accuracy: 0.3179\n",
      "Epoch 54/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4100 - accuracy: 0.3333 - val_loss: 1.4783 - val_accuracy: 0.3155\n",
      "Epoch 55/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4101 - accuracy: 0.3353 - val_loss: 1.4774 - val_accuracy: 0.3165\n",
      "Epoch 56/100\n",
      "133/133 [==============================] - 1s 4ms/step - loss: 1.4087 - accuracy: 0.3385 - val_loss: 1.4788 - val_accuracy: 0.3155\n",
      "Epoch 57/100\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.4081 - accuracy: 0.3341 - val_loss: 1.4799 - val_accuracy: 0.3165\n",
      "Epoch 58/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4113 - accuracy: 0.3316 - val_loss: 1.4809 - val_accuracy: 0.3170\n",
      "Epoch 59/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4081 - accuracy: 0.3341 - val_loss: 1.4805 - val_accuracy: 0.3188\n",
      "Epoch 60/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4112 - accuracy: 0.3264 - val_loss: 1.4811 - val_accuracy: 0.3165\n",
      "Epoch 61/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4090 - accuracy: 0.3307 - val_loss: 1.4815 - val_accuracy: 0.3146\n",
      "Epoch 62/100\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.4096 - accuracy: 0.3307 - val_loss: 1.4822 - val_accuracy: 0.3094\n",
      "Epoch 63/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4087 - accuracy: 0.3325 - val_loss: 1.4832 - val_accuracy: 0.3132\n",
      "Epoch 64/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4107 - accuracy: 0.3301 - val_loss: 1.4839 - val_accuracy: 0.3165\n",
      "Epoch 65/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4103 - accuracy: 0.3337 - val_loss: 1.4847 - val_accuracy: 0.3170\n",
      "Epoch 66/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4064 - accuracy: 0.3355 - val_loss: 1.4852 - val_accuracy: 0.3151\n",
      "Epoch 67/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4072 - accuracy: 0.3382 - val_loss: 1.4824 - val_accuracy: 0.3179\n",
      "Epoch 68/100\n",
      "133/133 [==============================] - 1s 4ms/step - loss: 1.4064 - accuracy: 0.3370 - val_loss: 1.4840 - val_accuracy: 0.3122\n",
      "Epoch 69/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4071 - accuracy: 0.3321 - val_loss: 1.4860 - val_accuracy: 0.3146\n",
      "Epoch 70/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4072 - accuracy: 0.3349 - val_loss: 1.4863 - val_accuracy: 0.3122\n",
      "Epoch 71/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4097 - accuracy: 0.3357 - val_loss: 1.4847 - val_accuracy: 0.3122\n",
      "Epoch 72/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4053 - accuracy: 0.3364 - val_loss: 1.4867 - val_accuracy: 0.3127\n",
      "Epoch 73/100\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.4060 - accuracy: 0.3312 - val_loss: 1.4879 - val_accuracy: 0.3146\n",
      "Epoch 74/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4036 - accuracy: 0.3398 - val_loss: 1.4897 - val_accuracy: 0.3099\n",
      "Epoch 75/100\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.4069 - accuracy: 0.3364 - val_loss: 1.4891 - val_accuracy: 0.3122\n",
      "Epoch 76/100\n",
      "133/133 [==============================] - 1s 4ms/step - loss: 1.4066 - accuracy: 0.3342 - val_loss: 1.4904 - val_accuracy: 0.3137\n",
      "Epoch 77/100\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.4078 - accuracy: 0.3379 - val_loss: 1.4911 - val_accuracy: 0.3113\n",
      "Epoch 78/100\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.4075 - accuracy: 0.3355 - val_loss: 1.4895 - val_accuracy: 0.3122\n",
      "Epoch 79/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4070 - accuracy: 0.3331 - val_loss: 1.4897 - val_accuracy: 0.3066\n",
      "Epoch 80/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4056 - accuracy: 0.3370 - val_loss: 1.4944 - val_accuracy: 0.3099\n",
      "Epoch 81/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4050 - accuracy: 0.3327 - val_loss: 1.4946 - val_accuracy: 0.3179\n",
      "Epoch 82/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4049 - accuracy: 0.3367 - val_loss: 1.4954 - val_accuracy: 0.3094\n",
      "Epoch 83/100\n",
      "133/133 [==============================] - 1s 4ms/step - loss: 1.4053 - accuracy: 0.3342 - val_loss: 1.4921 - val_accuracy: 0.3179\n",
      "Epoch 84/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4067 - accuracy: 0.3328 - val_loss: 1.4903 - val_accuracy: 0.3127\n",
      "Epoch 85/100\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.4085 - accuracy: 0.3368 - val_loss: 1.4890 - val_accuracy: 0.3146\n",
      "Epoch 86/100\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.4020 - accuracy: 0.3379 - val_loss: 1.4950 - val_accuracy: 0.3127\n",
      "Epoch 87/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4044 - accuracy: 0.3335 - val_loss: 1.4954 - val_accuracy: 0.3051\n",
      "Epoch 88/100\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.4062 - accuracy: 0.3328 - val_loss: 1.4968 - val_accuracy: 0.3108\n",
      "Epoch 89/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4062 - accuracy: 0.3360 - val_loss: 1.4967 - val_accuracy: 0.3160\n",
      "Epoch 90/100\n",
      "133/133 [==============================] - 1s 4ms/step - loss: 1.4037 - accuracy: 0.3427 - val_loss: 1.4967 - val_accuracy: 0.3122\n",
      "Epoch 91/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4022 - accuracy: 0.3409 - val_loss: 1.5008 - val_accuracy: 0.3155\n",
      "Epoch 92/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4046 - accuracy: 0.3385 - val_loss: 1.4994 - val_accuracy: 0.3141\n",
      "Epoch 93/100\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.4070 - accuracy: 0.3341 - val_loss: 1.4979 - val_accuracy: 0.3155\n",
      "Epoch 94/100\n",
      "133/133 [==============================] - 1s 5ms/step - loss: 1.4065 - accuracy: 0.3328 - val_loss: 1.4979 - val_accuracy: 0.3160\n",
      "Epoch 95/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4062 - accuracy: 0.3370 - val_loss: 1.5009 - val_accuracy: 0.3193\n",
      "Epoch 96/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4034 - accuracy: 0.3409 - val_loss: 1.5007 - val_accuracy: 0.3094\n",
      "Epoch 97/100\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.4049 - accuracy: 0.3402 - val_loss: 1.4987 - val_accuracy: 0.3155\n",
      "Epoch 98/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4034 - accuracy: 0.3383 - val_loss: 1.5001 - val_accuracy: 0.3118\n",
      "Epoch 99/100\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.4057 - accuracy: 0.3325 - val_loss: 1.4991 - val_accuracy: 0.3165\n",
      "Epoch 100/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4055 - accuracy: 0.3369 - val_loss: 1.5006 - val_accuracy: 0.3188\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 1.5006 - accuracy: 0.3188\n",
      "*** Dev loss: 1.5006243270986221 - accuracy: 0.3188474178314209\n",
      "Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/claudio/anaconda3/envs/MachineLearning/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py:364: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  if isinstance(inputs, collections.Sequence):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All operations completed\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    args = read_args()\n",
    "    dataset, dev_dataset, test_dataset = load_dataset(args.dataset_dir, args.batch_size)\n",
    "    nlabels = dataset[TARGET_COL].unique().shape[0]\n",
    "    \n",
    "    # It's important to always use the same one-hot length\n",
    "    one_hot_columns = {\n",
    "        one_hot_col: dataset[one_hot_col].max()\n",
    "        for one_hot_col in args.one_hot_cols\n",
    "    }\n",
    "    embedded_columns = {\n",
    "        embedded_col: dataset[embedded_col].max() + 1\n",
    "        for embedded_col in args.embedded_cols\n",
    "    }\n",
    "    numeric_columns = args.numeric_cols\n",
    "    \n",
    "    # TODO shuffle the train dataset! (ready)\n",
    "    from sklearn.utils import shuffle\n",
    "    dataset = shuffle(dataset, random_state=22)\n",
    "\n",
    "    # TODO (optional) put these three types of columns in the same dictionary with \"column types\" (ready)\n",
    "    X_train, y_train = process_features(dataset, one_hot_columns, numeric_columns, embedded_columns)\n",
    "    direct_features_input_shape = (X_train['direct_features'].shape[1],)\n",
    "    X_dev, y_dev = process_features(dev_dataset, one_hot_columns, numeric_columns, embedded_columns)\n",
    "    \n",
    "    # Create the tensorflow Dataset\n",
    "    batch_size = 64\n",
    "    \n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size)\n",
    "    dev_ds   = tf.data.Dataset.from_tensor_slices((X_dev, y_dev)).batch(batch_size)\n",
    "    test_ds  = tf.data.Dataset.from_tensor_slices(process_features(\n",
    "        test_dataset, one_hot_columns, numeric_columns, embedded_columns, test=True)[0]).batch(batch_size)\n",
    "\n",
    "    # TODO: Build the Keras model (Ready)\n",
    "    model = define_model(embedded_columns, X_train['direct_features'], args.hidden_layer_sizes,\n",
    "                               args.dropout, nlabels)\n",
    "        \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Weight classes\n",
    "    from sklearn.utils import class_weight\n",
    "    import numpy as np\n",
    "    class_weight = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(dataset.AdoptionSpeed.values),\n",
    "                                                 dataset.AdoptionSpeed.values)\n",
    "    \n",
    "    # TODO: Fit the model (Ready)\n",
    "    mlflow.set_experiment(args.experiment_name)\n",
    "    print(\"Running: \", args.run_name)\n",
    "    with mlflow.start_run(nested=True, run_name=args.run_name):\n",
    "        # Log model hiperparameters first\n",
    "        mlflow.log_param('hidden_layer_size', args.hidden_layer_sizes)\n",
    "        mlflow.log_param('dropout', args.dropout)\n",
    "        mlflow.log_param('embedded_columns', embedded_columns)\n",
    "        mlflow.log_param('one_hot_columns', one_hot_columns)\n",
    "        mlflow.log_param('numerical_columns', numeric_columns)\n",
    "        mlflow.log_param('total_columns', numeric_columns + list(one_hot_columns.keys()) + list(embedded_columns.keys()))\n",
    "        mlflow.log_param('epochs', args.epochs)\n",
    "        mlflow.log_param('n_params', model.count_params())\n",
    "\n",
    "        # Train\n",
    "        history = model.fit(train_ds, epochs=args.epochs,\n",
    "                            validation_data=dev_ds, verbose=1, class_weight=class_weight)\n",
    "                            \n",
    "\n",
    "        # TODO: analyze history to see if model converges/overfits\n",
    "        \n",
    "        # TODO: Evaluate the model, calculating the metrics.\n",
    "        # Option 1: Use the model.evaluate() method. For this, the model must be\n",
    "        # already compiled with the metrics.\n",
    "        #performance = model.evaluate(X_test, y_test) #(Ready)\n",
    "\n",
    "        #loss, accuracy = 0, 0\n",
    "        loss, accuracy = model.evaluate(dev_ds)\n",
    "        print(\"*** Dev loss: {} - accuracy: {}\".format(loss, accuracy))\n",
    "        mlflow.log_metric('dev_loss', loss)\n",
    "        mlflow.log_metric('dev_accuracy', accuracy)\n",
    "        for epoch in range(args.epochs):\n",
    "            mlflow.log_metric('hist_train_accuracy', value=history.history['accuracy'][epoch], step=epoch)\n",
    "            mlflow.log_metric('hist_val_accuracy', value=history.history['val_accuracy'][epoch], step=epoch)\n",
    "            mlflow.log_metric('hist_train_loss', value=history.history['loss'][epoch], step=epoch)\n",
    "            mlflow.log_metric('hist_val_loss', value=history.history['val_loss'][epoch], step=epoch)\n",
    "\n",
    "        # Option 2: Use the model.predict() method and calculate the metrics using\n",
    "        # sklearn. We recommend this, because you can store the predictions if\n",
    "        # you need more analysis later. Also, if you calculate the metrics on a\n",
    "        # notebook, then you can compare multiple classifiers.\n",
    "        \n",
    "        #predictions = 'No prediction yet'\n",
    "        print(\"Predictions\")\n",
    "        predictions = model.predict(test_ds)\n",
    "        \n",
    "        # TODO: Convert predictions to classes\n",
    "        # TODO: Save the results for submission\n",
    "        # ...\n",
    "        #print(collections.Counter(predictions))\n",
    "        test_dataset[\"AdoptionSpeed\"] = predictions.argmax(axis=1)\n",
    "        test_dataset.to_csv(\"./submission.csv\", index=False, columns=[\"PID\", \"AdoptionSpeed\"])\n",
    "        \n",
    "    print('All operations completed')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
