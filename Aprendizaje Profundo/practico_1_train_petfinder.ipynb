{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\"\"\"Exercise 1\n",
    "\n",
    "Usage:\n",
    "\n",
    "$ CUDA_VISIBLE_DEVICES=2 python practico_1_train_petfinder.py --dataset_dir ../ --epochs 30 --dropout 0.1 0.1 --hidden_layer_sizes 200 100\n",
    "\n",
    "To know which GPU to use, you can check it with the command\n",
    "\n",
    "$ nvidia-smi\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Práctico 1 Aprendizaje Profundo de: Claudio Sarate - Martin Hunziker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import mlflow\n",
    "import numpy\n",
    "import pandas\n",
    "import easydict\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from itertools import zip_longest\n",
    "from collections import Iterable, Iterator\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "in_colab = 'google.colab' in sys.modules\n",
    "if in_colab:\n",
    "    !pip install tensorflow==2.0.0\n",
    "    !pip install mlflow\n",
    "    print(tf.__version__)\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    BASE_DIR = \"/content/drive/My Drive/Colab Notebooks/Machine Learning/Aprendizaje Profundo/Data/\"\n",
    "else:\n",
    "    BASE_DIR = \"./Data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "target_col = 'AdoptionSpeed'\n",
    "embed_size = 5\n",
    "dropòuts = 0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "os.environ.keys()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "'jupyter_notebook' in sys.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def read_args():\n",
    "   \n",
    "    #parser = argparse.ArgumentParser(\n",
    "\n",
    "#    description='Training a MLP on the petfinder dataset')\n",
    "    # Here you have some examples of classifier parameters. You can add\n",
    "    # more arguments or change these if you need to.\n",
    "#    parser.add_argument('--dataset_dir', default=BASE_DIR, type=str,\n",
    "#                        help='Directory with the training and test files.')\n",
    "#    parser.add_argument('--hidden_layer_sizes', nargs='+', default=[100], type=int,\n",
    "#                        help='Number of hidden units of each hidden layer.')\n",
    "#    parser.add_argument('--epochs', default=10, type=int,\n",
    "#                        help='Number of epochs to train.')\n",
    "#    parser.add_argument('--dropout', nargs='+', default=[0.5], type=float,\n",
    "#                        help='Dropout ratio for every layer.')\n",
    "#    parser.add_argument('--batch_size', type=int, default=64,\n",
    "#                        help='Number of instances in each batch.')\n",
    "#    parser.add_argument('--one_hot_cols', type=str, nargs='+', default=['Gender', 'Color1'],\n",
    "#                        help='One hot columns.')\n",
    "#    parser.add_argument('--numeric_cols', type=str, nargs='+', default=['Age', 'Fee'],\n",
    "#                        help='Numeric columns.')\n",
    "#    parser.add_argument('--embedded_cols', type=str, nargs='+', default=['Breed1'],\n",
    "#                        help='Embedded columns.')\n",
    "#    parser.add_argument('--experiment_name', type=str, default='Base model',\n",
    "#                        help='Name of the experiment, used in mlflow.')\n",
    "#    parser.add_argument('--run_name', type=str, default='run00',\n",
    "#                        help='Name of the run, used in mlflow.')\n",
    "#    args = parser.parse_args()\n",
    "\n",
    "#    assert len(args.hidden_layer_sizes) == len(args.dropout)\n",
    "#    return args\n",
    "\n",
    "    #Cambio para correr en jupyter notebook con argumentos que se definen dentro\n",
    "    args = easydict.EasyDict({'dataset_dir': BASE_DIR,\n",
    "                              'hidden_layer_sizes': [10, 100],\n",
    "                              'epochs': 100,\n",
    "                              'dropout': [0.3, 0.5],\n",
    "                              'batch_size': 64,\n",
    "                              'one_hot_cols': ['Gender', 'Color1'],\n",
    "                              'numeric_cols': ['Age', 'Fee'],\n",
    "                              'embedded_cols': ['Breed1'],\n",
    "                              'experiment_name': 'Base Model',\n",
    "                              'run_name': 'run01'})\n",
    "\n",
    "    assert len(args.hidden_layer_sizes) == len(args.dropout)\n",
    "    print (args)\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def process_features(df, one_hot_columns, numeric_columns, embedded_columns, test=False):\n",
    "    direct_features = []\n",
    "\n",
    "    # Create one hot encodings\n",
    "    for one_hot_col, max_value in one_hot_columns.items():\n",
    "        direct_features.append(tf.keras.utils.to_categorical(df[one_hot_col] - 1, max_value))\n",
    "\n",
    "    # TODO Create and append numeric columns\n",
    "    for column in numeric_columns:\n",
    "        direct_features.append(tf.keras.utils.normalize(df[column].values.reshape(-1,1)))\n",
    "\n",
    "#    Otras Alternativas\n",
    "\n",
    "#        scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "#        scaler.fit(df[column].values.reshape(-1, 1))\n",
    "#        df[column] = scaler.transform(df[column].values.reshape(-1,1)).reshape(-1)\n",
    "#    for col in df[numeric_columns]:\n",
    "#        direct_features.append(df[numeric_columns].values)   \n",
    "\n",
    "#    Otra alternativa\n",
    "#        scaler_features = preprocessing.StandardScaler().fit_transform(df[[num_column]].values)\n",
    "#        direct_features.append(scaler_features)\n",
    "\n",
    "    # Concatenate all features that don't need further embedding into a single matrix.\n",
    "  \n",
    "    features = {'direct_features': numpy.hstack(direct_features)}\n",
    "\n",
    "    # Create embedding columns - nothing to do here. We will use the zero embedding for OOV\n",
    "    for embedded_col in embedded_columns.keys():\n",
    "        features[embedded_col] = df[embedded_col].values\n",
    "\n",
    "    if not test:\n",
    "        nlabels = df[target_col].unique().shape[0]\n",
    "        # Convert labels to one-hot encodings\n",
    "        targets = tf.keras.utils.to_categorical(df[target_col], nlabels)\n",
    "    else:\n",
    "        targets = None\n",
    "    \n",
    "    return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def load_dataset(dataset_dir, batch_size):\n",
    "\n",
    "    # Read train dataset (and maybe dev, if you need to...)\n",
    "    dataset, dev_dataset = train_test_split(\n",
    "        pandas.read_csv(os.path.join(dataset_dir, 'train.csv')), test_size=0.2)\n",
    "    \n",
    "    test_dataset = pandas.read_csv(os.path.join(dataset_dir, 'test.csv'))\n",
    "    \n",
    "    print('Training samples {}, test_samples {}'.format(\n",
    "        dataset.shape[0], test_dataset.shape[0]))\n",
    "    \n",
    "    return dataset, dev_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def define_model(embedded_input, direct_input, hidden_layers, dropouts, nlabels, init_seed):\n",
    "    tf.keras.backend.clear_session()\n",
    "    initializer = tf.keras.initializers.glorot_uniform(seed=init_seed)\n",
    "\n",
    "\n",
    "    # Add one input and one embedding for each embedded column\n",
    "    embedding_layers = []\n",
    "    inputs           = []\n",
    "    for embedded_col, max_value in embedded_input.items():\n",
    "        input_layer = layers.Input(shape=(1,), name=embedded_col)\n",
    "        inputs.append(input_layer)\n",
    "        # Define the embedding layer\n",
    "        embedding_size = int(max_value / embed_size)\n",
    "        embedding_layers.append(\n",
    "            tf.squeeze(layers.Embedding(input_dim=max_value, output_dim=embedding_size)(input_layer), axis=-2))\n",
    "        print('Adding embedding of size {} for layer {}'.format(embedding_size, embedded_col))\n",
    "\n",
    "    # Add the direct features already calculated\n",
    "    direct_features_input_shape = direct_input.shape[1]\n",
    "    direct_features_input = layers.Input(shape=direct_features_input_shape, name='direct_features')\n",
    "    inputs.append(direct_features_input)\n",
    "\n",
    "    # Concatenate everything together\n",
    "    features = layers.concatenate(embedding_layers + [direct_features_input])\n",
    "    \n",
    "    # Creating Models\n",
    "    n_layers = len(hidden_layers)\n",
    "    if len(dropouts) > n_layers:\n",
    "        dropouts = dropouts[:n_layers]\n",
    "        \n",
    "    for n_neurons, drop, layer in zip_longest(hidden_layers, dropouts, range(n_layers)):\n",
    "        if layer == 0:\n",
    "            dense      = layers.Dense(n_neurons, activation='relu', kernel_initializer=initializer)(features)\n",
    "            last_layer = dense\n",
    "        else:\n",
    "            dense      = layers.Dense(n_neurons, activation='relu', kernel_initializer=initializer)(last_layer)\n",
    "            last_layer = dense\n",
    "        if drop is not None:\n",
    "            #drop_layer = layers.BatchNormalization()(last_layer)\n",
    "            #last_layer = drop_layer\n",
    "            drop_layer = layers.Dropout(drop)(last_layer)\n",
    "            last_layer = drop_layer\n",
    "    \n",
    "    output_layer = layers.Dense(nlabels, activation='softmax')(last_layer)\n",
    "    model        = models.Model(inputs=inputs, outputs=output_layer)            \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_dir': './Data/', 'hidden_layer_sizes': [10, 100], 'epochs': 100, 'dropout': [0.3, 0.5], 'batch_size': 64, 'one_hot_cols': ['Gender', 'Color1'], 'numeric_cols': ['Age', 'Fee'], 'embedded_cols': ['Breed1'], 'experiment_name': 'Base Model', 'run_name': 'run01'}\n",
      "Training samples 8465, test_samples 4411\n",
      "Adding embedding of size 61 for layer Breed1\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Breed1 (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 61)        18788       Breed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Squeeze (TensorFlow [(None, 61)]         0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "direct_features (InputLayer)    [(None, 12)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 73)           0           tf_op_layer_Squeeze[0][0]        \n",
      "                                                                 direct_features[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10)           740         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 10)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          1100        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 5)            505         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 21,133\n",
      "Trainable params: 21,133\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Running:  run01_0\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 1.5155 - accuracy: 0.2637 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.4635 - accuracy: 0.2868 - val_loss: 1.4637 - val_accuracy: 0.3118\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4575 - accuracy: 0.2897 - val_loss: 1.4626 - val_accuracy: 0.3155\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.4499 - accuracy: 0.2997 - val_loss: 1.4622 - val_accuracy: 0.3174\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.4473 - accuracy: 0.2946 - val_loss: 1.4615 - val_accuracy: 0.3160\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4412 - accuracy: 0.3181 - val_loss: 1.4606 - val_accuracy: 0.3170\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4398 - accuracy: 0.3125 - val_loss: 1.4605 - val_accuracy: 0.3174\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4349 - accuracy: 0.3174 - val_loss: 1.4584 - val_accuracy: 0.3132\n",
      "Epoch 9/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.4326 - accuracy: 0.3230 - val_loss: 1.4588 - val_accuracy: 0.3132\n",
      "Epoch 10/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4344 - accuracy: 0.3154 - val_loss: 1.4604 - val_accuracy: 0.3141\n",
      "Epoch 11/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4320 - accuracy: 0.3239 - val_loss: 1.4616 - val_accuracy: 0.3160\n",
      "Epoch 12/100\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.4286 - accuracy: 0.3250 - val_loss: 1.4619 - val_accuracy: 0.3165\n",
      "Epoch 13/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4309 - accuracy: 0.3278 - val_loss: 1.4625 - val_accuracy: 0.3151\n",
      "Epoch 14/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4241 - accuracy: 0.3289 - val_loss: 1.4619 - val_accuracy: 0.3155\n",
      "Epoch 15/100\n",
      "133/133 [==============================] - 1s 5ms/step - loss: 1.4266 - accuracy: 0.3247 - val_loss: 1.4605 - val_accuracy: 0.3160\n",
      "Epoch 16/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.4253 - accuracy: 0.3257 - val_loss: 1.4614 - val_accuracy: 0.3118\n",
      "Epoch 17/100\n",
      "133/133 [==============================] - 1s 5ms/step - loss: 1.4240 - accuracy: 0.3259 - val_loss: 1.4637 - val_accuracy: 0.3141\n",
      "Epoch 18/100\n",
      "133/133 [==============================] - 1s 5ms/step - loss: 1.4229 - accuracy: 0.3288 - val_loss: 1.4645 - val_accuracy: 0.3137\n",
      "Epoch 19/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4220 - accuracy: 0.3285 - val_loss: 1.4645 - val_accuracy: 0.3151\n",
      "Epoch 20/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4213 - accuracy: 0.3305 - val_loss: 1.4648 - val_accuracy: 0.3179\n",
      "Epoch 21/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4211 - accuracy: 0.3351 - val_loss: 1.4656 - val_accuracy: 0.3127\n",
      "Epoch 22/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4160 - accuracy: 0.3380 - val_loss: 1.4658 - val_accuracy: 0.3207\n",
      "Epoch 23/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4171 - accuracy: 0.3309 - val_loss: 1.4649 - val_accuracy: 0.3137\n",
      "Epoch 24/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4198 - accuracy: 0.3314 - val_loss: 1.4652 - val_accuracy: 0.3118\n",
      "Epoch 25/100\n",
      "133/133 [==============================] - 1s 5ms/step - loss: 1.4163 - accuracy: 0.3348 - val_loss: 1.4692 - val_accuracy: 0.3127\n",
      "Epoch 26/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4195 - accuracy: 0.3309 - val_loss: 1.4678 - val_accuracy: 0.3113\n",
      "Epoch 27/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4162 - accuracy: 0.3356 - val_loss: 1.4691 - val_accuracy: 0.3127\n",
      "Epoch 28/100\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.4157 - accuracy: 0.3335 - val_loss: 1.4715 - val_accuracy: 0.3122\n",
      "Epoch 29/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4147 - accuracy: 0.3318 - val_loss: 1.4733 - val_accuracy: 0.3141\n",
      "Epoch 30/100\n",
      "133/133 [==============================] - 1s 4ms/step - loss: 1.4113 - accuracy: 0.3408 - val_loss: 1.4727 - val_accuracy: 0.3132\n",
      "Epoch 31/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.4134 - accuracy: 0.3342 - val_loss: 1.4738 - val_accuracy: 0.3141\n",
      "Epoch 32/100\n",
      "133/133 [==============================] - 1s 4ms/step - loss: 1.4150 - accuracy: 0.3312 - val_loss: 1.4736 - val_accuracy: 0.3198\n",
      "Epoch 33/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.4141 - accuracy: 0.3357 - val_loss: 1.4750 - val_accuracy: 0.3141\n",
      "Epoch 34/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4128 - accuracy: 0.3325 - val_loss: 1.4775 - val_accuracy: 0.3127\n",
      "Epoch 35/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4104 - accuracy: 0.3446 - val_loss: 1.4782 - val_accuracy: 0.3118\n",
      "Epoch 36/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4100 - accuracy: 0.3395 - val_loss: 1.4782 - val_accuracy: 0.3099\n",
      "Epoch 37/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4126 - accuracy: 0.3363 - val_loss: 1.4774 - val_accuracy: 0.3141\n",
      "Epoch 38/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4148 - accuracy: 0.3379 - val_loss: 1.4765 - val_accuracy: 0.3132\n",
      "Epoch 39/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4077 - accuracy: 0.3380 - val_loss: 1.4802 - val_accuracy: 0.3151\n",
      "Epoch 40/100\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.4093 - accuracy: 0.3392 - val_loss: 1.4816 - val_accuracy: 0.3151\n",
      "Epoch 41/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4120 - accuracy: 0.3398 - val_loss: 1.4842 - val_accuracy: 0.3122\n",
      "Epoch 42/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4114 - accuracy: 0.3440 - val_loss: 1.4837 - val_accuracy: 0.3165\n",
      "Epoch 43/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4085 - accuracy: 0.3321 - val_loss: 1.4850 - val_accuracy: 0.3094\n",
      "Epoch 44/100\n",
      "133/133 [==============================] - 1s 4ms/step - loss: 1.4083 - accuracy: 0.3416 - val_loss: 1.4859 - val_accuracy: 0.3165\n",
      "Epoch 45/100\n",
      "133/133 [==============================] - 1s 4ms/step - loss: 1.4085 - accuracy: 0.3327 - val_loss: 1.4869 - val_accuracy: 0.3155\n",
      "Epoch 46/100\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.4102 - accuracy: 0.3374 - val_loss: 1.4872 - val_accuracy: 0.3122\n",
      "Epoch 47/100\n",
      "133/133 [==============================] - 1s 4ms/step - loss: 1.4072 - accuracy: 0.3421 - val_loss: 1.4858 - val_accuracy: 0.3122\n",
      "Epoch 48/100\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.4091 - accuracy: 0.3337 - val_loss: 1.4882 - val_accuracy: 0.3146\n",
      "Epoch 49/100\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.4091 - accuracy: 0.3380 - val_loss: 1.4875 - val_accuracy: 0.3141\n",
      "Epoch 50/100\n",
      "133/133 [==============================] - 1s 4ms/step - loss: 1.4083 - accuracy: 0.3398 - val_loss: 1.4883 - val_accuracy: 0.3160\n",
      "Epoch 51/100\n",
      "133/133 [==============================] - 1s 4ms/step - loss: 1.4092 - accuracy: 0.3405 - val_loss: 1.4876 - val_accuracy: 0.3141\n",
      "Epoch 52/100\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.4078 - accuracy: 0.3364 - val_loss: 1.4869 - val_accuracy: 0.3165\n",
      "Epoch 53/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.4081 - accuracy: 0.3392 - val_loss: 1.4901 - val_accuracy: 0.3160\n",
      "Epoch 54/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.4122 - accuracy: 0.3318 - val_loss: 1.4884 - val_accuracy: 0.3155\n",
      "Epoch 55/100\n",
      "133/133 [==============================] - 1s 4ms/step - loss: 1.4044 - accuracy: 0.3428 - val_loss: 1.4886 - val_accuracy: 0.3099\n",
      "Epoch 56/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.4079 - accuracy: 0.3376 - val_loss: 1.4897 - val_accuracy: 0.3155\n",
      "Epoch 57/100\n",
      "133/133 [==============================] - 1s 5ms/step - loss: 1.4062 - accuracy: 0.3395 - val_loss: 1.4913 - val_accuracy: 0.3099\n",
      "Epoch 58/100\n",
      "133/133 [==============================] - 1s 5ms/step - loss: 1.4044 - accuracy: 0.3418 - val_loss: 1.4933 - val_accuracy: 0.3146\n",
      "Epoch 59/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4087 - accuracy: 0.3400 - val_loss: 1.4927 - val_accuracy: 0.3141\n",
      "Epoch 60/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4057 - accuracy: 0.3442 - val_loss: 1.4968 - val_accuracy: 0.3141\n",
      "Epoch 61/100\n",
      "133/133 [==============================] - 1s 5ms/step - loss: 1.4081 - accuracy: 0.3367 - val_loss: 1.4975 - val_accuracy: 0.3141\n",
      "Epoch 62/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4074 - accuracy: 0.3451 - val_loss: 1.4964 - val_accuracy: 0.3165\n",
      "Epoch 63/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4051 - accuracy: 0.3408 - val_loss: 1.4971 - val_accuracy: 0.3141\n",
      "Epoch 64/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4059 - accuracy: 0.3441 - val_loss: 1.5005 - val_accuracy: 0.3122\n",
      "Epoch 65/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4078 - accuracy: 0.3395 - val_loss: 1.4954 - val_accuracy: 0.3118\n",
      "Epoch 66/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4054 - accuracy: 0.3422 - val_loss: 1.5000 - val_accuracy: 0.3170\n",
      "Epoch 67/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4027 - accuracy: 0.3355 - val_loss: 1.5003 - val_accuracy: 0.3122\n",
      "Epoch 68/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4059 - accuracy: 0.3427 - val_loss: 1.4997 - val_accuracy: 0.3103\n",
      "Epoch 69/100\n",
      "133/133 [==============================] - 1s 4ms/step - loss: 1.4032 - accuracy: 0.3396 - val_loss: 1.5017 - val_accuracy: 0.3103\n",
      "Epoch 70/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4071 - accuracy: 0.3369 - val_loss: 1.5013 - val_accuracy: 0.3118\n",
      "Epoch 71/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4039 - accuracy: 0.3448 - val_loss: 1.5034 - val_accuracy: 0.3080\n",
      "Epoch 72/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4083 - accuracy: 0.3375 - val_loss: 1.5005 - val_accuracy: 0.3108\n",
      "Epoch 73/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4053 - accuracy: 0.3398 - val_loss: 1.4991 - val_accuracy: 0.3174\n",
      "Epoch 74/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4032 - accuracy: 0.3381 - val_loss: 1.5016 - val_accuracy: 0.3132\n",
      "Epoch 75/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4040 - accuracy: 0.3425 - val_loss: 1.5061 - val_accuracy: 0.3099\n",
      "Epoch 76/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4042 - accuracy: 0.3418 - val_loss: 1.5015 - val_accuracy: 0.3132\n",
      "Epoch 77/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4080 - accuracy: 0.3357 - val_loss: 1.5034 - val_accuracy: 0.3118\n",
      "Epoch 78/100\n",
      "133/133 [==============================] - 1s 4ms/step - loss: 1.4001 - accuracy: 0.3433 - val_loss: 1.5069 - val_accuracy: 0.3160\n",
      "Epoch 79/100\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.4036 - accuracy: 0.3392 - val_loss: 1.5053 - val_accuracy: 0.3160\n",
      "Epoch 80/100\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.4015 - accuracy: 0.3433 - val_loss: 1.5067 - val_accuracy: 0.3137\n",
      "Epoch 81/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4078 - accuracy: 0.3392 - val_loss: 1.5050 - val_accuracy: 0.3113\n",
      "Epoch 82/100\n",
      "133/133 [==============================] - 1s 4ms/step - loss: 1.4065 - accuracy: 0.3353 - val_loss: 1.5016 - val_accuracy: 0.3118\n",
      "Epoch 83/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4041 - accuracy: 0.3434 - val_loss: 1.5072 - val_accuracy: 0.3137\n",
      "Epoch 84/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4049 - accuracy: 0.3416 - val_loss: 1.5101 - val_accuracy: 0.3151\n",
      "Epoch 85/100\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.4018 - accuracy: 0.3422 - val_loss: 1.5080 - val_accuracy: 0.3146\n",
      "Epoch 86/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4014 - accuracy: 0.3418 - val_loss: 1.5117 - val_accuracy: 0.3170\n",
      "Epoch 87/100\n",
      "133/133 [==============================] - 1s 4ms/step - loss: 1.4020 - accuracy: 0.3396 - val_loss: 1.5097 - val_accuracy: 0.3170\n",
      "Epoch 88/100\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.4081 - accuracy: 0.3375 - val_loss: 1.5102 - val_accuracy: 0.3193\n",
      "Epoch 89/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4037 - accuracy: 0.3395 - val_loss: 1.5121 - val_accuracy: 0.3174\n",
      "Epoch 90/100\n",
      "133/133 [==============================] - 1s 4ms/step - loss: 1.3999 - accuracy: 0.3459 - val_loss: 1.5140 - val_accuracy: 0.3137\n",
      "Epoch 91/100\n",
      "133/133 [==============================] - 1s 4ms/step - loss: 1.4001 - accuracy: 0.3385 - val_loss: 1.5127 - val_accuracy: 0.3184\n",
      "Epoch 92/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4032 - accuracy: 0.3415 - val_loss: 1.5137 - val_accuracy: 0.3217\n",
      "Epoch 93/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4020 - accuracy: 0.3412 - val_loss: 1.5118 - val_accuracy: 0.3188\n",
      "Epoch 94/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4034 - accuracy: 0.3439 - val_loss: 1.5125 - val_accuracy: 0.3155\n",
      "Epoch 95/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4020 - accuracy: 0.3388 - val_loss: 1.5143 - val_accuracy: 0.3170\n",
      "Epoch 96/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4046 - accuracy: 0.3422 - val_loss: 1.5171 - val_accuracy: 0.3174\n",
      "Epoch 97/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4013 - accuracy: 0.3433 - val_loss: 1.5195 - val_accuracy: 0.3165\n",
      "Epoch 98/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4056 - accuracy: 0.3429 - val_loss: 1.5164 - val_accuracy: 0.3170\n",
      "Epoch 99/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4013 - accuracy: 0.3479 - val_loss: 1.5190 - val_accuracy: 0.3146\n",
      "Epoch 100/100\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4028 - accuracy: 0.3388 - val_loss: 1.5152 - val_accuracy: 0.3127\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 1.5152 - accuracy: 0.3127\n",
      "*** Dev loss: 1.5152047101189108 - accuracy: 0.3127066493034363\n",
      "Predictions\n",
      "All operations completed\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "#    get_available_gpus()\n",
    "\n",
    "    args = read_args()\n",
    "    dataset, dev_dataset, test_dataset = load_dataset(args.dataset_dir, args.batch_size)\n",
    "    nlabels = dataset[target_col].unique().shape[0]\n",
    "    \n",
    "    # It's important to always use the same one-hot length\n",
    "    one_hot_columns = {\n",
    "        one_hot_col: dataset[one_hot_col].max()\n",
    "        for one_hot_col in args.one_hot_cols\n",
    "    }\n",
    "    embedded_columns = {\n",
    "        embedded_col: dataset[embedded_col].max() + 1\n",
    "        for embedded_col in args.embedded_cols\n",
    "    }\n",
    "    numeric_columns = args.numeric_cols\n",
    "    \n",
    "    # TODO shuffle the train dataset! (ready)\n",
    "    from sklearn.utils import shuffle\n",
    "    dataset = shuffle(dataset, random_state=22)\n",
    "\n",
    "    # TODO (optional) put these three types of columns in the same dictionary with \"column types\" (ready)\n",
    "    X_train, y_train = process_features(dataset, one_hot_columns, numeric_columns, embedded_columns)\n",
    "    direct_features_input_shape = (X_train['direct_features'].shape[1],)\n",
    "    X_dev, y_dev = process_features(dev_dataset, one_hot_columns, numeric_columns, embedded_columns)\n",
    "    \n",
    "    # Create the tensorflow Dataset\n",
    "    batch_size = 64\n",
    "    \n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size)\n",
    "    dev_ds   = tf.data.Dataset.from_tensor_slices((X_dev, y_dev)).batch(batch_size)\n",
    "    test_ds  = tf.data.Dataset.from_tensor_slices(process_features(\n",
    "        test_dataset, one_hot_columns, numeric_columns, embedded_columns, test=True)[0]).batch(batch_size)\n",
    "\n",
    "    # TODO: Build the Keras model (Ready)\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    for n_model in range(1):\n",
    "        local_seed = np.random.randint(1000)\n",
    "        model = define_model(embedded_columns, X_train['direct_features'], args.hidden_layer_sizes,\n",
    "                                   args.dropout, nlabels, init_seed=local_seed)\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "        print(model.summary())\n",
    "        plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "        # Weight classes\n",
    "        from sklearn.utils import class_weight\n",
    "        class_weight = class_weight.compute_class_weight('balanced',\n",
    "                                                     np.unique(dataset.AdoptionSpeed.values),\n",
    "                                                     dataset.AdoptionSpeed.values)\n",
    "\n",
    "        # TODO: Fit the model (Ready)\n",
    "        mlflow.set_experiment(args.experiment_name)\n",
    "        run_name = args.run_name + \"_{}\".format(n_model)\n",
    "        print(\"Running: \", run_name, )\n",
    "        with mlflow.start_run(nested=True, run_name=run_name):\n",
    "            # Log model hiperparameters first\n",
    "            mlflow.log_param('hidden_layer_size', args.hidden_layer_sizes)\n",
    "            mlflow.log_param('dropout', args.dropout)\n",
    "            mlflow.log_param('embedded_columns', embedded_columns)\n",
    "            mlflow.log_param('one_hot_columns', one_hot_columns)\n",
    "            mlflow.log_param('numerical_columns', numeric_columns)\n",
    "            mlflow.log_param('total_columns', numeric_columns + list(one_hot_columns.keys()) + list(embedded_columns.keys()))\n",
    "            mlflow.log_param('epochs', args.epochs)\n",
    "            mlflow.log_param('n_params', model.count_params())\n",
    "            mlflow.log_param('run_name', args.run_name)\n",
    "\n",
    "            # Train\n",
    "            history = model.fit(train_ds, epochs=args.epochs,\n",
    "                                validation_data=dev_ds, verbose=1, class_weight=class_weight)\n",
    "\n",
    "\n",
    "            # TODO: analyze history to see if model converges/overfits\n",
    "\n",
    "            # TODO: Evaluate the model, calculating the metrics.\n",
    "            # Option 1: Use the model.evaluate() method. For this, the model must be\n",
    "            # already compiled with the metrics.\n",
    "            #performance = model.evaluate(X_test, y_test) #(Ready)\n",
    "\n",
    "            #loss, accuracy = 0, 0\n",
    "            loss, accuracy = model.evaluate(dev_ds)\n",
    "            print(\"*** Dev loss: {} - accuracy: {}\".format(loss, accuracy))\n",
    "            mlflow.log_metric('dev_loss', loss)\n",
    "            mlflow.log_metric('dev_accuracy', accuracy)\n",
    "            for epoch in range(args.epochs):\n",
    "                mlflow.log_metric('hist_train_accuracy', value=history.history['accuracy'][epoch], step=epoch)\n",
    "                mlflow.log_metric('hist_val_accuracy', value=history.history['val_accuracy'][epoch], step=epoch)\n",
    "                mlflow.log_metric('hist_train_loss', value=history.history['loss'][epoch], step=epoch)\n",
    "                mlflow.log_metric('hist_val_loss', value=history.history['val_loss'][epoch], step=epoch)\n",
    "\n",
    "            # Option 2: Use the model.predict() method and calculate the metrics using\n",
    "            # sklearn. We recommend this, because you can store the predictions if\n",
    "            # you need more analysis later. Also, if you calculate the metrics on a\n",
    "            # notebook, then you can compare multiple classifiers.\n",
    "\n",
    "            #predictions = 'No prediction yet'\n",
    "            print(\"Predictions\")\n",
    "            predictions = model.predict(test_ds)\n",
    "\n",
    "            # TODO: Convert predictions to classes\n",
    "            # TODO: Save the results for submission\n",
    "            # ...\n",
    "            #print(collections.Counter(predictions))\n",
    "            test_dataset[\"AdoptionSpeed\"] = predictions.argmax(axis=1)\n",
    "            test_dataset.to_csv(\"./submission.csv\", index=False, columns=[\"PID\", \"AdoptionSpeed\"])\n",
    "        \n",
    "    print('All operations completed')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
