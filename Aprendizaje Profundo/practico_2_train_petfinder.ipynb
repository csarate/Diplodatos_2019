{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{},"version_major":2,"version_minor":0}},"colab":{"name":"practico_2_train_petfinder.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"utczXWEwQSjI","colab_type":"text"},"source":["# Práctico 2 - Redes en escalera avanzadas"]},{"cell_type":"code","metadata":{"Collapsed":"false","scrolled":true,"id":"qdki57WyQSjO","colab_type":"code","outputId":"69f0e678-657c-46af-c233-14e84b9a04dc","executionInfo":{"status":"ok","timestamp":1574907191523,"user_tz":180,"elapsed":2347,"user":{"displayName":"Claudio Sárate","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAyvJcBOeDmirReKch-IfaEhczbBFhZkXiSlm968g=s64","userId":"01992612555747043556"}},"colab":{"base_uri":"https://localhost:8080/","height":84}},"source":["import sys \n","import io\n","import nltk\n","import numpy as np\n","import os\n","import pandas as pd\n","import sklearn\n","import numpy as np\n","import tensorflow as tf\n","\n","from tensorflow.keras import regularizers\n","from IPython.display import SVG\n","from gensim import corpora\n","from nltk import word_tokenize\n","from nltk.corpus import stopwords\n","from pprint import pprint\n","from sklearn.preprocessing import MinMaxScaler\n","\n","nltk.download([\"punkt\", \"stopwords\"]);"],"execution_count":63,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"nriY43u0QSjT","colab_type":"code","outputId":"12cfbdfe-77fa-4dba-b2f3-544c6ee67528","executionInfo":{"status":"ok","timestamp":1574907208150,"user_tz":180,"elapsed":18926,"user":{"displayName":"Claudio Sárate","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAyvJcBOeDmirReKch-IfaEhczbBFhZkXiSlm968g=s64","userId":"01992612555747043556"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["in_colab = 'google.colab' in sys.modules\n","if in_colab:\n","    !pip install --upgrade pip\n","    !pip install --upgrade tensorflow-gpu==2.0.0\n","    !pip install --upgrade mlflow graphviz pydot\n","\n","    print(tf.__version__)\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    BASE_DIR = \"/content/drive/My Drive/Colab Notebooks/Machine Learning/Aprendizaje Profundo/Data/\"\n","else:\n","    BASE_DIR = \"./Data/\""],"execution_count":64,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: pip in /usr/local/lib/python3.6/dist-packages (19.3.1)\n","Requirement already up-to-date: tensorflow-gpu==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n","Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.12.0)\n","Requirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.0.8)\n","Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.1.8)\n","Requirement already satisfied, skipping upgrade: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.2.2)\n","Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.1.0)\n","Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.33.6)\n","Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n","Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.15.0)\n","Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.17.4)\n","Requirement already satisfied, skipping upgrade: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (2.0.2)\n","Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (2.0.1)\n","Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.8.0)\n","Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.8.1)\n","Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n","Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.10.0)\n","Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.11.2)\n","Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0) (2.8.0)\n","Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.7.1)\n","Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.1.1)\n","Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (41.6.0)\n","Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2.21.0)\n","Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.4.1)\n","Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.16.0)\n","Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.2.7)\n","Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (4.0)\n","Requirement already satisfied, skipping upgrade: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.1.1)\n","Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2.8)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2019.9.11)\n","Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.0.4)\n","Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.24.3)\n","Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.3.0)\n","Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.4.7)\n","Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.1.0)\n","Requirement already up-to-date: mlflow in /usr/local/lib/python3.6/dist-packages (1.4.0)\n","Requirement already up-to-date: graphviz in /usr/local/lib/python3.6/dist-packages (0.13.2)\n","Requirement already up-to-date: pydot in /usr/local/lib/python3.6/dist-packages (1.4.1)\n","Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from mlflow) (3.13)\n","Requirement already satisfied, skipping upgrade: sqlalchemy in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.3.11)\n","Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from mlflow) (0.25.3)\n","Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.12.0)\n","Requirement already satisfied, skipping upgrade: requests>=2.17.3 in /usr/local/lib/python3.6/dist-packages (from mlflow) (2.21.0)\n","Requirement already satisfied, skipping upgrade: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from mlflow) (7.0)\n","Requirement already satisfied, skipping upgrade: python-dateutil in /usr/local/lib/python3.6/dist-packages (from mlflow) (2.6.1)\n","Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.17.4)\n","Requirement already satisfied, skipping upgrade: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from mlflow) (3.10.0)\n","Requirement already satisfied, skipping upgrade: databricks-cli>=0.8.7 in /usr/local/lib/python3.6/dist-packages (from mlflow) (0.9.1)\n","Requirement already satisfied, skipping upgrade: querystring-parser in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.2.4)\n","Requirement already satisfied, skipping upgrade: cloudpickle in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.2.2)\n","Requirement already satisfied, skipping upgrade: sqlparse in /usr/local/lib/python3.6/dist-packages (from mlflow) (0.3.0)\n","Requirement already satisfied, skipping upgrade: gorilla in /usr/local/lib/python3.6/dist-packages (from mlflow) (0.3.0)\n","Requirement already satisfied, skipping upgrade: docker>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from mlflow) (4.1.0)\n","Requirement already satisfied, skipping upgrade: entrypoints in /usr/local/lib/python3.6/dist-packages (from mlflow) (0.3)\n","Requirement already satisfied, skipping upgrade: gitpython>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from mlflow) (3.0.5)\n","Requirement already satisfied, skipping upgrade: alembic in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.3.1)\n","Requirement already satisfied, skipping upgrade: Flask in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.1.1)\n","Requirement already satisfied, skipping upgrade: simplejson in /usr/local/lib/python3.6/dist-packages (from mlflow) (3.17.0)\n","Requirement already satisfied, skipping upgrade: gunicorn; platform_system != \"Windows\" in /usr/local/lib/python3.6/dist-packages (from mlflow) (20.0.0)\n","Requirement already satisfied, skipping upgrade: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from pydot) (2.4.5)\n","Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->mlflow) (2018.9)\n","Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.17.3->mlflow) (2.8)\n","Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.17.3->mlflow) (3.0.4)\n","Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.17.3->mlflow) (1.24.3)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.17.3->mlflow) (2019.9.11)\n","Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.0->mlflow) (41.6.0)\n","Requirement already satisfied, skipping upgrade: configparser>=0.3.5 in /usr/local/lib/python3.6/dist-packages (from databricks-cli>=0.8.7->mlflow) (4.0.2)\n","Requirement already satisfied, skipping upgrade: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from databricks-cli>=0.8.7->mlflow) (0.8.5)\n","Requirement already satisfied, skipping upgrade: websocket-client>=0.32.0 in /usr/local/lib/python3.6/dist-packages (from docker>=4.0.0->mlflow) (0.56.0)\n","Requirement already satisfied, skipping upgrade: gitdb2>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from gitpython>=2.1.0->mlflow) (2.0.6)\n","Requirement already satisfied, skipping upgrade: python-editor>=0.3 in /usr/local/lib/python3.6/dist-packages (from alembic->mlflow) (1.0.4)\n","Requirement already satisfied, skipping upgrade: Mako in /usr/local/lib/python3.6/dist-packages (from alembic->mlflow) (1.1.0)\n","Requirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask->mlflow) (0.16.0)\n","Requirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask->mlflow) (1.1.0)\n","Requirement already satisfied, skipping upgrade: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask->mlflow) (2.10.3)\n","Requirement already satisfied, skipping upgrade: smmap2>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from gitdb2>=2.0.0->gitpython>=2.1.0->mlflow) (2.0.5)\n","Requirement already satisfied, skipping upgrade: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic->mlflow) (1.1.1)\n","2.0.0\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"17niph0yQSjW","colab_type":"code","outputId":"79b09d76-aaea-479d-c160-f8678eb7ad7b","executionInfo":{"status":"ok","timestamp":1574907208151,"user_tz":180,"elapsed":18913,"user":{"displayName":"Claudio Sárate","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAyvJcBOeDmirReKch-IfaEhczbBFhZkXiSlm968g=s64","userId":"01992612555747043556"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["try:\n","    # %tensorflow_version only exists in Colab.\n","    %tensorflow_version 2.x\n","except Exception:\n","    pass\n","import tensorflow as tf\n","\n","from tensorflow.keras import layers, models"],"execution_count":65,"outputs":[{"output_type":"stream","text":["TensorFlow is already loaded. Please restart the runtime to change versions.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"eBgk14sSQSja","colab_type":"text"},"source":["## Carga de los datos"]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"iIyuzqVtQSjb","colab_type":"code","outputId":"321ea71f-9eaf-4df6-b4b8-9a995806cdf5","executionInfo":{"status":"ok","timestamp":1574907208153,"user_tz":180,"elapsed":18906,"user":{"displayName":"Claudio Sárate","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAyvJcBOeDmirReKch-IfaEhczbBFhZkXiSlm968g=s64","userId":"01992612555747043556"}},"colab":{"base_uri":"https://localhost:8080/","height":467}},"source":["dataset = pd.read_csv(os.path.join(BASE_DIR, 'train.csv'))\n","\n","target_col = 'AdoptionSpeed'\n","nlabels = dataset[target_col].unique().shape[0]\n","\n","dataset.head(5)"],"execution_count":66,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Type</th>\n","      <th>Age</th>\n","      <th>Breed1</th>\n","      <th>Breed2</th>\n","      <th>Gender</th>\n","      <th>Color1</th>\n","      <th>Color2</th>\n","      <th>Color3</th>\n","      <th>MaturitySize</th>\n","      <th>FurLength</th>\n","      <th>Vaccinated</th>\n","      <th>Dewormed</th>\n","      <th>Sterilized</th>\n","      <th>Health</th>\n","      <th>Quantity</th>\n","      <th>Fee</th>\n","      <th>State</th>\n","      <th>Description</th>\n","      <th>AdoptionSpeed</th>\n","      <th>PID</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>299</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>100</td>\n","      <td>41326</td>\n","      <td>Nibble is a 3+ month old ball of cuteness. He ...</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>307</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>150</td>\n","      <td>41401</td>\n","      <td>Good guard dog, very alert, active, obedience ...</td>\n","      <td>2</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>307</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>41326</td>\n","      <td>This handsome yet cute boy is up for adoption....</td>\n","      <td>2</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>266</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>41326</td>\n","      <td>This is a stray kitten that came to my house. ...</td>\n","      <td>2</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2</td>\n","      <td>12</td>\n","      <td>264</td>\n","      <td>264</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>300</td>\n","      <td>41326</td>\n","      <td>anyone within the area of ipoh or taiping who ...</td>\n","      <td>1</td>\n","      <td>6</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Type  Age  ...  AdoptionSpeed  PID\n","0     2    3  ...              2    0\n","1     1    4  ...              2    3\n","2     1    1  ...              2    4\n","3     2    3  ...              2    5\n","4     2   12  ...              1    6\n","\n","[5 rows x 20 columns]"]},"metadata":{"tags":[]},"execution_count":66}]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"tkHy79ZBQSjf","colab_type":"text"},"source":["## Preproceso del texto para agregarlo como feature (manejo de secuencias)\n","\n","A diferencia del práctico anterior, en este caso es necesario utilizar el texto como feature extra. Pueden luego agregarlo a una red recurrente o convolucional y concatenar su salida a los atributos \"escalares\" (como \"raza\" o \"género\").\n","\n","A continuación les mostraremos los pasos a seguir para ello. La descripción detallada de para que sirve cada paso se encuentra disponible en el [notebook 3](./3_cnns.ipynb).\n","\n","### Tokenización"]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"6om_cnbsQSjg","colab_type":"code","colab":{}},"source":["SW = set(stopwords.words(\"english\"))\n","\n","def tokenize_description(description):\n","    return [w.lower() for w in word_tokenize(description, language=\"english\") if w.lower() not in SW]\n","\n","# Fill the null values with the empty string to avoid errors with NLTK tokenization\n","dataset[\"TokenizedDescription\"] = dataset[\"Description\"].fillna(value=\"\").apply(tokenize_description)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"DQq29S6IQSjj","colab_type":"text"},"source":["#### Tamaño de las descripciones\n","\n","Un punto importante a tener en cuenta es que las descripciones tienen tamaño variable, y esto no es compatible con los algoritmos de aprendizaje automático. Por lo que hay que llevar las secuencias a un tamaño uniforme.\n","\n","Para definir dicho tamaño uniforme, es útil mirar qué tamaños mínimos, máximos y medios manejan las descripciones y a partir de esto establecer el tamaño máximo de la secuencia."]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"rZCyYTMHQSjk","colab_type":"code","outputId":"9eec3ebb-e205-4290-ffe1-e1ec2ba3f7d0","executionInfo":{"status":"ok","timestamp":1574907215686,"user_tz":180,"elapsed":26420,"user":{"displayName":"Claudio Sárate","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAyvJcBOeDmirReKch-IfaEhczbBFhZkXiSlm968g=s64","userId":"01992612555747043556"}},"colab":{"base_uri":"https://localhost:8080/","height":168}},"source":["pprint(dataset[\"TokenizedDescription\"].apply(len).describe())"],"execution_count":68,"outputs":[{"output_type":"stream","text":["count    10582.000000\n","mean        44.418541\n","std         48.464623\n","min          0.000000\n","25%         16.000000\n","50%         31.000000\n","75%         55.000000\n","max        803.000000\n","Name: TokenizedDescription, dtype: float64\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"B6oXoz5HQSjn","colab_type":"text"},"source":["Vemos que más del 75% de las secuencias tienen 55 palabras o menos. Esto es un buen punto de partida, así que podemos establecer el tamaño máximo de las secuencia en 55 palabras."]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"PLamyB6RQSjo","colab_type":"code","colab":{}},"source":["MAX_SEQUENCE_LEN = 55"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"caZkXZLLQSjr","colab_type":"text"},"source":["## Vocabulario"]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"1M-rRF6YQSjs","colab_type":"code","colab":{}},"source":["vocabulary = corpora.Dictionary(dataset[\"TokenizedDescription\"])\n","vocabulary.filter_extremes(no_below=1, no_above=1.0, keep_n=10000)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"c1hM-FpZQSjv","colab_type":"text"},"source":["## Word Embeddings (GloVe)"]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"JWcnBXamQSjw","colab_type":"code","outputId":"0b5e2f92-8b84-403b-81a3-4af9e4a2ee98","executionInfo":{"status":"ok","timestamp":1574907220050,"user_tz":180,"elapsed":30764,"user":{"displayName":"Claudio Sárate","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAyvJcBOeDmirReKch-IfaEhczbBFhZkXiSlm968g=s64","userId":"01992612555747043556"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["embeddings_index = {}\n","\n","with open(BASE_DIR+\"glove.6B.100d.txt\", \"r\") as fh:\n","    for line in fh:\n","        values = line.split()\n","        word = values[0]\n","        if word in vocabulary.token2id:  # Only use the embeddings of words in our vocabulary\n","            coefs = np.asarray(values[1:], dtype='float32')\n","            embeddings_index[word] = coefs\n","\n","print(\"Found {} word vectors.\".format(len(embeddings_index)))"],"execution_count":71,"outputs":[{"output_type":"stream","text":["Found 7897 word vectors.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"8KHwSBYoQSj0","colab_type":"text"},"source":["## Creación de los datasets\n","\n","Similar al práctico anterior, tendremos datos que serán \"one-hot-encoded\", otros serán \"embeddings\" y otros serán numéricos.\n","\n","El caso particular del texto es que será tratado como una secuencia de embeddings, y dichos embeddings no serán entrenados en conjunto con la red, sino que serán tomados de un modelo \"pre-entrenado\". En este caso utilizamos GloVe, pero podríamos haber utilizado otro modelo (e.g. FastText)."]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"u4ra_8PNQSj1","colab_type":"code","colab":{}},"source":["# It's important to always use the same one-hot length\n","one_hot_columns = {\n","    one_hot_col: dataset[one_hot_col].max()\n","    for one_hot_col in ['Type', 'Gender', 'MaturitySize', 'FurLength', \n","                        'Vaccinated', 'Dewormed', 'Sterilized', 'Health']\n","}\n","embedded_columns = {\n","    embedded_col: dataset[embedded_col].max() + 1\n","    for embedded_col in ['Breed1', 'Breed2', 'Color1', 'Color2', 'Color3']\n","}\n","numeric_columns = ['Age', 'Fee']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"y5_mAUnfQSj4","colab_type":"text"},"source":["## Generador del conjunto de datos\n","\n","Dada la naturaleza de los datos de texto, y que estos representan una secuencia de datos (que se da luego a una red recurrente o convolucional), en este caso no crearemos los datasets de antemano, sino que los generaremos a medida que el algoritmo de entrenamiento los pida. \n","\n","En particular, es porque las secuencias de texto pueden no tener el mismo tamaño (las oraciones tienen diferente cantidad de palabras), pero para que los modelos de redes las acepten, necesitamos rellenarlas (*padding*) de manera que todas tengan el mismo tamaño.\n","\n","En este paso también vamos a truncar aquellas secuencias de descripciones con más de `MAX_SEQUENCE_LEN` palabras, de manera que al hacer uso de `padded_batch` no lance un error al encontrarse con secuencias de tamaño mayor."]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"cuGkAwN0QSj4","colab_type":"code","colab":{}},"source":["# Normalizamos las vareiables numéricas\n","dataset[numeric_columns] = sklearn.preprocessing.minmax_scale(dataset[numeric_columns], feature_range=(0, 1), axis=0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"W3SG4KoRQSj8","colab_type":"code","colab":{}},"source":["def dataset_generator(ds, test_data=False):\n","    for _, row in ds.iterrows():\n","        instance = {}\n","        \n","        direct_features = []\n","        # One hot encoded features\n","        instance[\"direct_features\"] = np.hstack(\n","            [tf.keras.utils.to_categorical(row[one_hot_col] - 1, max_value)\n","            for one_hot_col, max_value in one_hot_columns.items()])\n","\n","        # Numeric features (should be normalized beforehand)\n","        for n_col in numeric_columns:\n","            instance[n_col] = [row[n_col]]\n"," \n","    #   for n_col in numeric_columns:\n","    #       instance[n_col] = tf.keras.utils.normalize(ds[n_col].values.reshape(-1,1))\n","    #   for n_col in numeric_columns:\n","    #       instance[n_col] = np.hstack(tf.keras.utils.normalize(df[n_col].values)) \n","\n","\n","        # Embedded features\n","        for embedded_col in embedded_columns:\n","            instance[embedded_col] = [row[embedded_col]]\n","        \n","        # Document to indices for text data, truncated at MAX_SEQUENCE_LEN words\n","        instance[\"description\"] = vocabulary.doc2idx(\n","            row[\"TokenizedDescription\"],\n","            unknown_word_index=len(vocabulary)\n","        )[:MAX_SEQUENCE_LEN]\n","        \n","        # One hot encoded target for categorical crossentropy\n","        if not test_data:\n","            target = tf.keras.utils.to_categorical(row[target_col], nlabels)\n","            yield instance, target\n","        else:\n","            yield instance\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"Rur1XmxzQSj-","colab_type":"code","outputId":"7506c3e4-860c-4531-ca56-20f996dc4eb0","executionInfo":{"status":"ok","timestamp":1574907220054,"user_tz":180,"elapsed":30598,"user":{"displayName":"Claudio Sárate","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAyvJcBOeDmirReKch-IfaEhczbBFhZkXiSlm968g=s64","userId":"01992612555747043556"}},"colab":{"base_uri":"https://localhost:8080/","height":538}},"source":["# Set output types of the generator (for numeric types check the type is valid)\n","instance_types = {\n","    \"direct_features\": tf.float32,\n","    \"description\": tf.int32\n","}\n","\n","for embedded_col in embedded_columns:\n","    instance_types[embedded_col] = tf.int32\n","\n","for n_col in numeric_columns:\n","    instance_types[n_col] = tf.float32\n","\n","tf_dataset = tf.data.Dataset.from_generator(\n","    lambda: dataset_generator(dataset),\n","    output_types=(instance_types, tf.int32)\n",")\n","\n","for data, target in tf_dataset.take(2):\n","    pprint(data)\n","    pprint(target)\n","    print()"],"execution_count":75,"outputs":[{"output_type":"stream","text":["{'Age': <tf.Tensor: id=8331, shape=(1,), dtype=float32, numpy=array([0.01176471], dtype=float32)>,\n"," 'Breed1': <tf.Tensor: id=8332, shape=(1,), dtype=int32, numpy=array([299], dtype=int32)>,\n"," 'Breed2': <tf.Tensor: id=8333, shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n"," 'Color1': <tf.Tensor: id=8334, shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n"," 'Color2': <tf.Tensor: id=8335, shape=(1,), dtype=int32, numpy=array([7], dtype=int32)>,\n"," 'Color3': <tf.Tensor: id=8336, shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n"," 'Fee': <tf.Tensor: id=8337, shape=(1,), dtype=float32, numpy=array([0.03333334], dtype=float32)>,\n"," 'description': <tf.Tensor: id=8338, shape=(42,), dtype=int32, numpy=\n","array([23,  2, 20, 24,  4, 10,  1, 11, 26,  1, 27,  9,  6, 21,  3,  8, 15,\n","       22, 33,  7, 13, 30,  1, 29, 18, 17,  1, 12, 31, 14,  5,  6, 16,  1,\n","       19, 28, 25, 32, 23,  0,  5,  1], dtype=int32)>,\n"," 'direct_features': <tf.Tensor: id=8339, shape=(24,), dtype=float32, numpy=\n","array([0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1.,\n","       0., 0., 1., 0., 1., 0., 0.], dtype=float32)>}\n","<tf.Tensor: id=8340, shape=(5,), dtype=int32, numpy=array([0, 0, 1, 0, 0], dtype=int32)>\n","\n","{'Age': <tf.Tensor: id=8341, shape=(1,), dtype=float32, numpy=array([0.01568628], dtype=float32)>,\n"," 'Breed1': <tf.Tensor: id=8342, shape=(1,), dtype=int32, numpy=array([307], dtype=int32)>,\n"," 'Breed2': <tf.Tensor: id=8343, shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n"," 'Color1': <tf.Tensor: id=8344, shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n"," 'Color2': <tf.Tensor: id=8345, shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>,\n"," 'Color3': <tf.Tensor: id=8346, shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n"," 'Fee': <tf.Tensor: id=8347, shape=(1,), dtype=float32, numpy=array([0.05], dtype=float32)>,\n"," 'description': <tf.Tensor: id=8348, shape=(24,), dtype=int32, numpy=\n","array([41, 42, 40, 35, 37, 35, 36, 35, 45, 50, 41, 44, 35, 46, 38, 48, 39,\n","       47, 15, 43, 35, 49, 34, 34], dtype=int32)>,\n"," 'direct_features': <tf.Tensor: id=8349, shape=(24,), dtype=float32, numpy=\n","array([1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.,\n","       0., 0., 1., 0., 1., 0., 0.], dtype=float32)>}\n","<tf.Tensor: id=8350, shape=(5,), dtype=int32, numpy=array([0, 0, 1, 0, 0], dtype=int32)>\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"VhtAelBQQSkC","colab_type":"text"},"source":["## Datos de entrenamiento y validación\n","\n","Ya generado el conjunto de datos base, tenemos que dividirlo en entrenamiento y validación. Además, como vamos a utilizar algunos datos que forman secuencias, los lotes (*batches*) de datos deben estar \"rellenados\" (*padded_batch*). \n","\n","Si bien rellenaremos \"todos\" los atributos, en la práctica el único que efectivamente se rellenará es el de *description* pues es el único con tamaños distintos."]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"9jED-TDTQSkD","colab_type":"code","colab":{}},"source":["TRAIN_SIZE = int(dataset.shape[0] * 0.8)\n","DEV_SIZE = dataset.shape[0] - TRAIN_SIZE\n","BATCH_SIZE = 128\n","\n","shuffled_dataset = tf_dataset.shuffle(TRAIN_SIZE + DEV_SIZE, seed=42)\n","\n","# Pad the datasets to the max value for all the \"non sequence\" features\n","padding_shapes = (\n","    {k: [-1] for k in [\"direct_features\"] + list(embedded_columns.keys()) + list([(numeric_columns)[0]]) + list([(numeric_columns)[1]])},\n","    [-1]\n",")\n","\n","# Pad to MAX_SEQUENCE_LEN for sequence features\n","padding_shapes[0][\"description\"] = [MAX_SEQUENCE_LEN]\n","\n","# Pad values are irrelevant for non padded data\n","padding_values = (\n","    {k: 0 for k in list(embedded_columns.keys())},\n","    0\n",")\n","\n","# Padding value for direct features should be a float\n","padding_values[0][\"direct_features\"] = np.float32(0)\n","\n","# Padding value for sequential features is the vocabulary length + 1\n","padding_values[0][\"description\"] = len(vocabulary) + 1\n","\n","# Agregamos para las columnas numéricas\n","padding_values[0][numeric_columns[0]] = np.float32(0)\n","padding_values[0][numeric_columns[1]] = np.float32(0)\n","\n","\n","train_dataset = shuffled_dataset.skip(DEV_SIZE)\\\n","    .padded_batch(BATCH_SIZE, padded_shapes=padding_shapes, padding_values=padding_values)\n","\n","dev_dataset = shuffled_dataset.take(DEV_SIZE)\\\n","    .padded_batch(BATCH_SIZE, padded_shapes=padding_shapes, padding_values=padding_values)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"dVk-0c5KQSkG","colab_type":"text"},"source":["## Construyendo el modelo\n","\n","Al modelo anterior tenemos que agregarle la capa que maneje los embeddings de las palabras, e inicializarla de manera acorde, podemos guiarnos por lo visto en el [notebook 3](./3_cnns.ipynb) para hacer esto.\n","\n","### Matriz de embeddings de palabras"]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"VNbTK4fzQSkH","colab_type":"code","colab":{}},"source":["EMBEDDINGS_DIM = 100  # Given by the model (in this case glove.6B.100d)\n","\n","embedding_matrix = np.zeros((len(vocabulary) + 2, 100))\n","\n","for widx, word in vocabulary.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[widx] = embedding_vector\n","    else:\n","        # Random normal initialization for words without embeddings\n","        embedding_matrix[widx] = np.random.normal(size=(100,))  \n","\n","# Random normal initialization for unknown words\n","embedding_matrix[len(vocabulary)] = np.random.normal(size=(100,))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"fCqumyD9QSkK","colab_type":"text"},"source":["### Definiendo los inputs del modelo\n","\n","Definamos los inputs del modelo, con el agregado de la capa de embeddings de palabras inicializada en `embedding_matrix`."]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"4iBB3sClQSkL","colab_type":"code","outputId":"e90de9ce-5989-40fa-bce0-4313f27e6aa1","executionInfo":{"status":"ok","timestamp":1574907220056,"user_tz":180,"elapsed":30580,"user":{"displayName":"Claudio Sárate","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAyvJcBOeDmirReKch-IfaEhczbBFhZkXiSlm968g=s64","userId":"01992612555747043556"}},"colab":{"base_uri":"https://localhost:8080/","height":101}},"source":["tf.keras.backend.clear_session()\n","\n","# Add one input and one embedding for each embedded column\n","embedding_layers = []\n","numeric_layer = []\n","inputs = []\n","for embedded_col, max_value in embedded_columns.items():\n","    input_layer = tf.keras.layers.Input(shape=(1,), name=embedded_col)\n","    inputs.append(input_layer)\n","    # Define the embedding layer\n","    embedding_size = int(max_value / 4)\n","    embedding_layers.append(\n","        tf.squeeze(\n","            tf.keras.layers.Embedding(\n","                input_dim=max_value, \n","                output_dim=embedding_size\n","            )(input_layer), \n","            axis=-2\n","        )\n","    )\n","    print('Adding embedding of size {} for layer {}'.format(embedding_size, embedded_col))\n","\n","# Add the direct features already calculated\n","direct_features_input = tf.keras.layers.Input(\n","    shape=(sum(one_hot_columns.values()),), \n","    name='direct_features'\n",")\n","inputs.append(direct_features_input)\n","\n","# Add numeric columns\n","numerical_inputs = [tf.keras.layers.Input(shape=(1,), name=n_col) for n_col in numeric_columns]\n","inputs += numerical_inputs\n","\n","# Word embedding layer\n","description_input = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LEN,), name=\"description\")\n","inputs.append(description_input)\n","\n","word_embeddings_layer = tf.keras.layers.Embedding(\n","    embedding_matrix.shape[0],\n","    EMBEDDINGS_DIM,\n","    weights=[embedding_matrix],\n","    input_length=MAX_SEQUENCE_LEN,\n","    trainable=False,\n","    name=\"word_embedding\"\n",")(description_input)"],"execution_count":78,"outputs":[{"output_type":"stream","text":["Adding embedding of size 77 for layer Breed1\n","Adding embedding of size 77 for layer Breed2\n","Adding embedding of size 2 for layer Color1\n","Adding embedding of size 2 for layer Color2\n","Adding embedding of size 2 for layer Color3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"S3KbDrQEQSkO","colab_type":"text"},"source":["### Definiendo la red que trabajará con el texto\n","\n","Antes de generar el *feature map* final entre los inputs y las clases, tenemos que generar el *feature map* de las secuencias de texto. \n"]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"r3ecSfyDQSkP","colab_type":"code","colab":{}},"source":["## TODO: Create a NN (CNN or RNN) for the description input (replace the next)\n","DESCRIPTION_FEATURES_LAYER_SIZE = 512\n","\n","description_features = tf.keras.layers.SimpleRNN(256)(word_embeddings_layer)\n","description_features = tf.keras.layers.Flatten()(word_embeddings_layer)  # This is a simple concatenation\n","description_features = tf.keras.layers.Dense(\n","    units=DESCRIPTION_FEATURES_LAYER_SIZE, \n","    activation=\"relu\", \n","    name=\"description_features\")(description_features)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"rF7jzuxCQSkU","colab_type":"text"},"source":["### Definiendo el *feature map* final de la red\n","\n","Ahora que tenemos nuestra representación de las descripciones, pasamos a combinarlo con los demás features en la última parte de nuestra red."]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"FIeaiJwoQSkW","colab_type":"code","colab":{}},"source":["HIDDEN_LAYER_SIZE = 128\n","\n","feature_map = tf.keras.layers.Concatenate(name=\"feature_map\")(\n","    embedding_layers + numerical_inputs +  [description_features, direct_features_input]\n",")\n","\n","hidden_layer = tf.keras.layers.Dense(HIDDEN_LAYER_SIZE, activation=\"relu\",kernel_regularizer=regularizers.l2(0.001))(feature_map)\n","hidden_layer = tf.keras.layers.BatchNormalization()(hidden_layer)\n","hidden_layer = tf.keras.layers.Dropout(0.3)(hidden_layer)\n","hidden_layer = tf.keras.layers.Dense(HIDDEN_LAYER_SIZE/2, activation=\"relu\",kernel_regularizer=regularizers.l2(0.001))(hidden_layer)\n","hidden_layer=tf.keras.layers.BatchNormalization()(hidden_layer)\n","hidden_layer=tf.keras.layers.Dropout(0.3)(hidden_layer)\n","\n","output_layer = tf.keras.layers.Dense(nlabels, activation=\"softmax\", name=\"output\")(hidden_layer)\n","\n","model = tf.keras.models.Model(inputs=inputs, outputs=[output_layer], name=\"amazing_model\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"Iqa4_gMiQSkb","colab_type":"text"},"source":["### Compilando y visualizando el modelo"]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"E3Co5cjJQSkc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"89b10087-27ef-45ac-b52f-6b577bd7c3b1","executionInfo":{"status":"ok","timestamp":1574907220506,"user_tz":180,"elapsed":31010,"user":{"displayName":"Claudio Sárate","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAyvJcBOeDmirReKch-IfaEhczbBFhZkXiSlm968g=s64","userId":"01992612555747043556"}}},"source":["model.compile(loss='categorical_crossentropy', \n","              optimizer='nadam',\n","              metrics=['accuracy'])\n","model.summary()"],"execution_count":81,"outputs":[{"output_type":"stream","text":["Model: \"amazing_model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","description (InputLayer)        [(None, 55)]         0                                            \n","__________________________________________________________________________________________________\n","Breed1 (InputLayer)             [(None, 1)]          0                                            \n","__________________________________________________________________________________________________\n","Breed2 (InputLayer)             [(None, 1)]          0                                            \n","__________________________________________________________________________________________________\n","Color1 (InputLayer)             [(None, 1)]          0                                            \n","__________________________________________________________________________________________________\n","Color2 (InputLayer)             [(None, 1)]          0                                            \n","__________________________________________________________________________________________________\n","Color3 (InputLayer)             [(None, 1)]          0                                            \n","__________________________________________________________________________________________________\n","word_embedding (Embedding)      (None, 55, 100)      1000200     description[0][0]                \n","__________________________________________________________________________________________________\n","embedding (Embedding)           (None, 1, 77)        23716       Breed1[0][0]                     \n","__________________________________________________________________________________________________\n","embedding_1 (Embedding)         (None, 1, 77)        23716       Breed2[0][0]                     \n","__________________________________________________________________________________________________\n","embedding_2 (Embedding)         (None, 1, 2)         16          Color1[0][0]                     \n","__________________________________________________________________________________________________\n","embedding_3 (Embedding)         (None, 1, 2)         16          Color2[0][0]                     \n","__________________________________________________________________________________________________\n","embedding_4 (Embedding)         (None, 1, 2)         16          Color3[0][0]                     \n","__________________________________________________________________________________________________\n","flatten (Flatten)               (None, 5500)         0           word_embedding[0][0]             \n","__________________________________________________________________________________________________\n","tf_op_layer_Squeeze (TensorFlow [(None, 77)]         0           embedding[0][0]                  \n","__________________________________________________________________________________________________\n","tf_op_layer_Squeeze_1 (TensorFl [(None, 77)]         0           embedding_1[0][0]                \n","__________________________________________________________________________________________________\n","tf_op_layer_Squeeze_2 (TensorFl [(None, 2)]          0           embedding_2[0][0]                \n","__________________________________________________________________________________________________\n","tf_op_layer_Squeeze_3 (TensorFl [(None, 2)]          0           embedding_3[0][0]                \n","__________________________________________________________________________________________________\n","tf_op_layer_Squeeze_4 (TensorFl [(None, 2)]          0           embedding_4[0][0]                \n","__________________________________________________________________________________________________\n","Age (InputLayer)                [(None, 1)]          0                                            \n","__________________________________________________________________________________________________\n","Fee (InputLayer)                [(None, 1)]          0                                            \n","__________________________________________________________________________________________________\n","description_features (Dense)    (None, 512)          2816512     flatten[0][0]                    \n","__________________________________________________________________________________________________\n","direct_features (InputLayer)    [(None, 24)]         0                                            \n","__________________________________________________________________________________________________\n","feature_map (Concatenate)       (None, 698)          0           tf_op_layer_Squeeze[0][0]        \n","                                                                 tf_op_layer_Squeeze_1[0][0]      \n","                                                                 tf_op_layer_Squeeze_2[0][0]      \n","                                                                 tf_op_layer_Squeeze_3[0][0]      \n","                                                                 tf_op_layer_Squeeze_4[0][0]      \n","                                                                 Age[0][0]                        \n","                                                                 Fee[0][0]                        \n","                                                                 description_features[0][0]       \n","                                                                 direct_features[0][0]            \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 128)          89472       feature_map[0][0]                \n","__________________________________________________________________________________________________\n","batch_normalization (BatchNorma (None, 128)          512         dense[0][0]                      \n","__________________________________________________________________________________________________\n","dropout (Dropout)               (None, 128)          0           batch_normalization[0][0]        \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 64)           8256        dropout[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 64)           256         dense_1[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 64)           0           batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","output (Dense)                  (None, 5)            325         dropout_1[0][0]                  \n","==================================================================================================\n","Total params: 3,963,013\n","Trainable params: 2,962,429\n","Non-trainable params: 1,000,584\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"F3ZGJ5iuQSkf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":984},"outputId":"b3560e77-5c12-4f2e-8e49-4128aa652763","executionInfo":{"status":"ok","timestamp":1574907220507,"user_tz":180,"elapsed":31003,"user":{"displayName":"Claudio Sárate","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAyvJcBOeDmirReKch-IfaEhczbBFhZkXiSlm968g=s64","userId":"01992612555747043556"}}},"source":["SVG(tf.keras.utils.model_to_dot(model, dpi=60).create(prog='dot', format='svg'))"],"execution_count":82,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<IPython.core.display.SVG object>"],"image/svg+xml":"<svg height=\"707pt\" viewBox=\"0.00 0.00 2111.00 848.00\" width=\"1759pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(.8333 .8333) rotate(0) translate(4 844)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-844 2107,-844 2107,4 -4,4\" stroke=\"transparent\"/>\n<!-- 140364738437416 -->\n<g class=\"node\" id=\"node1\">\n<title>140364738437416</title>\n<polygon fill=\"none\" points=\"21.5,-803.5 21.5,-839.5 172.5,-839.5 172.5,-803.5 21.5,-803.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"97\" y=\"-817.8\">description: InputLayer</text>\n</g>\n<!-- 140364738424560 -->\n<g class=\"node\" id=\"node7\">\n<title>140364738424560</title>\n<polygon fill=\"none\" points=\"0,-730.5 0,-766.5 194,-766.5 194,-730.5 0,-730.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"97\" y=\"-744.8\">word_embedding: Embedding</text>\n</g>\n<!-- 140364738437416&#45;&gt;140364738424560 -->\n<g class=\"edge\" id=\"edge1\">\n<title>140364738437416-&gt;140364738424560</title>\n<path d=\"M97,-803.4551C97,-795.3828 97,-785.6764 97,-776.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"100.5001,-776.5903 97,-766.5904 93.5001,-776.5904 100.5001,-776.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140364737856792 -->\n<g class=\"node\" id=\"node2\">\n<title>140364737856792</title>\n<polygon fill=\"none\" points=\"269.5,-730.5 269.5,-766.5 398.5,-766.5 398.5,-730.5 269.5,-730.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"334\" y=\"-744.8\">Breed1: InputLayer</text>\n</g>\n<!-- 140364738558160 -->\n<g class=\"node\" id=\"node8\">\n<title>140364738558160</title>\n<polygon fill=\"none\" points=\"256,-657.5 256,-693.5 412,-693.5 412,-657.5 256,-657.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"334\" y=\"-671.8\">embedding: Embedding</text>\n</g>\n<!-- 140364737856792&#45;&gt;140364738558160 -->\n<g class=\"edge\" id=\"edge2\">\n<title>140364737856792-&gt;140364738558160</title>\n<path d=\"M334,-730.4551C334,-722.3828 334,-712.6764 334,-703.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"337.5001,-703.5903 334,-693.5904 330.5001,-703.5904 337.5001,-703.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140364737975688 -->\n<g class=\"node\" id=\"node3\">\n<title>140364737975688</title>\n<polygon fill=\"none\" points=\"560.5,-730.5 560.5,-766.5 689.5,-766.5 689.5,-730.5 560.5,-730.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"625\" y=\"-744.8\">Breed2: InputLayer</text>\n</g>\n<!-- 140364737935064 -->\n<g class=\"node\" id=\"node9\">\n<title>140364737935064</title>\n<polygon fill=\"none\" points=\"539.5,-657.5 539.5,-693.5 710.5,-693.5 710.5,-657.5 539.5,-657.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"625\" y=\"-671.8\">embedding_1: Embedding</text>\n</g>\n<!-- 140364737975688&#45;&gt;140364737935064 -->\n<g class=\"edge\" id=\"edge3\">\n<title>140364737975688-&gt;140364737935064</title>\n<path d=\"M625,-730.4551C625,-722.3828 625,-712.6764 625,-703.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"628.5001,-703.5903 625,-693.5904 621.5001,-703.5904 628.5001,-703.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140364738484936 -->\n<g class=\"node\" id=\"node4\">\n<title>140364738484936</title>\n<polygon fill=\"none\" points=\"859,-730.5 859,-766.5 987,-766.5 987,-730.5 859,-730.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"923\" y=\"-744.8\">Color1: InputLayer</text>\n</g>\n<!-- 140364738581616 -->\n<g class=\"node\" id=\"node10\">\n<title>140364738581616</title>\n<polygon fill=\"none\" points=\"837.5,-657.5 837.5,-693.5 1008.5,-693.5 1008.5,-657.5 837.5,-657.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"923\" y=\"-671.8\">embedding_2: Embedding</text>\n</g>\n<!-- 140364738484936&#45;&gt;140364738581616 -->\n<g class=\"edge\" id=\"edge4\">\n<title>140364738484936-&gt;140364738581616</title>\n<path d=\"M923,-730.4551C923,-722.3828 923,-712.6764 923,-703.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"926.5001,-703.5903 923,-693.5904 919.5001,-703.5904 926.5001,-703.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140364738477136 -->\n<g class=\"node\" id=\"node5\">\n<title>140364738477136</title>\n<polygon fill=\"none\" points=\"1157,-730.5 1157,-766.5 1285,-766.5 1285,-730.5 1157,-730.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1221\" y=\"-744.8\">Color2: InputLayer</text>\n</g>\n<!-- 140364738543568 -->\n<g class=\"node\" id=\"node11\">\n<title>140364738543568</title>\n<polygon fill=\"none\" points=\"1135.5,-657.5 1135.5,-693.5 1306.5,-693.5 1306.5,-657.5 1135.5,-657.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1221\" y=\"-671.8\">embedding_3: Embedding</text>\n</g>\n<!-- 140364738477136&#45;&gt;140364738543568 -->\n<g class=\"edge\" id=\"edge5\">\n<title>140364738477136-&gt;140364738543568</title>\n<path d=\"M1221,-730.4551C1221,-722.3828 1221,-712.6764 1221,-703.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1224.5001,-703.5903 1221,-693.5904 1217.5001,-703.5904 1224.5001,-703.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140364738485328 -->\n<g class=\"node\" id=\"node6\">\n<title>140364738485328</title>\n<polygon fill=\"none\" points=\"1455,-730.5 1455,-766.5 1583,-766.5 1583,-730.5 1455,-730.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1519\" y=\"-744.8\">Color3: InputLayer</text>\n</g>\n<!-- 140364738412160 -->\n<g class=\"node\" id=\"node12\">\n<title>140364738412160</title>\n<polygon fill=\"none\" points=\"1433.5,-657.5 1433.5,-693.5 1604.5,-693.5 1604.5,-657.5 1433.5,-657.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1519\" y=\"-671.8\">embedding_4: Embedding</text>\n</g>\n<!-- 140364738485328&#45;&gt;140364738412160 -->\n<g class=\"edge\" id=\"edge6\">\n<title>140364738485328-&gt;140364738412160</title>\n<path d=\"M1519,-730.4551C1519,-722.3828 1519,-712.6764 1519,-703.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1522.5001,-703.5903 1519,-693.5904 1515.5001,-703.5904 1522.5001,-703.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140364738231264 -->\n<g class=\"node\" id=\"node13\">\n<title>140364738231264</title>\n<polygon fill=\"none\" points=\"48,-657.5 48,-693.5 146,-693.5 146,-657.5 48,-657.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"97\" y=\"-671.8\">flatten: Flatten</text>\n</g>\n<!-- 140364738424560&#45;&gt;140364738231264 -->\n<g class=\"edge\" id=\"edge7\">\n<title>140364738424560-&gt;140364738231264</title>\n<path d=\"M97,-730.4551C97,-722.3828 97,-712.6764 97,-703.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"100.5001,-703.5903 97,-693.5904 93.5001,-703.5904 100.5001,-703.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140364739336624 -->\n<g class=\"node\" id=\"node14\">\n<title>140364739336624</title>\n<polygon fill=\"none\" points=\"201.5,-584.5 201.5,-620.5 466.5,-620.5 466.5,-584.5 201.5,-584.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"334\" y=\"-598.8\">tf_op_layer_Squeeze: TensorFlowOpLayer</text>\n</g>\n<!-- 140364738558160&#45;&gt;140364739336624 -->\n<g class=\"edge\" id=\"edge8\">\n<title>140364738558160-&gt;140364739336624</title>\n<path d=\"M334,-657.4551C334,-649.3828 334,-639.6764 334,-630.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"337.5001,-630.5903 334,-620.5904 330.5001,-630.5904 337.5001,-630.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140364739337072 -->\n<g class=\"node\" id=\"node15\">\n<title>140364739337072</title>\n<polygon fill=\"none\" points=\"485,-584.5 485,-620.5 765,-620.5 765,-584.5 485,-584.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"625\" y=\"-598.8\">tf_op_layer_Squeeze_1: TensorFlowOpLayer</text>\n</g>\n<!-- 140364737935064&#45;&gt;140364739337072 -->\n<g class=\"edge\" id=\"edge9\">\n<title>140364737935064-&gt;140364739337072</title>\n<path d=\"M625,-657.4551C625,-649.3828 625,-639.6764 625,-630.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"628.5001,-630.5903 625,-620.5904 621.5001,-630.5904 628.5001,-630.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140364739343080 -->\n<g class=\"node\" id=\"node16\">\n<title>140364739343080</title>\n<polygon fill=\"none\" points=\"783,-584.5 783,-620.5 1063,-620.5 1063,-584.5 783,-584.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"923\" y=\"-598.8\">tf_op_layer_Squeeze_2: TensorFlowOpLayer</text>\n</g>\n<!-- 140364738581616&#45;&gt;140364739343080 -->\n<g class=\"edge\" id=\"edge10\">\n<title>140364738581616-&gt;140364739343080</title>\n<path d=\"M923,-657.4551C923,-649.3828 923,-639.6764 923,-630.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"926.5001,-630.5903 923,-620.5904 919.5001,-630.5904 926.5001,-630.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140364739311992 -->\n<g class=\"node\" id=\"node17\">\n<title>140364739311992</title>\n<polygon fill=\"none\" points=\"1081,-584.5 1081,-620.5 1361,-620.5 1361,-584.5 1081,-584.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1221\" y=\"-598.8\">tf_op_layer_Squeeze_3: TensorFlowOpLayer</text>\n</g>\n<!-- 140364738543568&#45;&gt;140364739311992 -->\n<g class=\"edge\" id=\"edge11\">\n<title>140364738543568-&gt;140364739311992</title>\n<path d=\"M1221,-657.4551C1221,-649.3828 1221,-639.6764 1221,-630.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1224.5001,-630.5903 1221,-620.5904 1217.5001,-630.5904 1224.5001,-630.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140364739304640 -->\n<g class=\"node\" id=\"node18\">\n<title>140364739304640</title>\n<polygon fill=\"none\" points=\"1379,-584.5 1379,-620.5 1659,-620.5 1659,-584.5 1379,-584.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1519\" y=\"-598.8\">tf_op_layer_Squeeze_4: TensorFlowOpLayer</text>\n</g>\n<!-- 140364738412160&#45;&gt;140364739304640 -->\n<g class=\"edge\" id=\"edge12\">\n<title>140364738412160-&gt;140364739304640</title>\n<path d=\"M1519,-657.4551C1519,-649.3828 1519,-639.6764 1519,-630.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1522.5001,-630.5903 1519,-620.5904 1515.5001,-630.5904 1522.5001,-630.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140364738145864 -->\n<g class=\"node\" id=\"node21\">\n<title>140364738145864</title>\n<polygon fill=\"none\" points=\"10.5,-584.5 10.5,-620.5 183.5,-620.5 183.5,-584.5 10.5,-584.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"97\" y=\"-598.8\">description_features: Dense</text>\n</g>\n<!-- 140364738231264&#45;&gt;140364738145864 -->\n<g class=\"edge\" id=\"edge13\">\n<title>140364738231264-&gt;140364738145864</title>\n<path d=\"M97,-657.4551C97,-649.3828 97,-639.6764 97,-630.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"100.5001,-630.5903 97,-620.5904 93.5001,-630.5904 100.5001,-630.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140364739332136 -->\n<g class=\"node\" id=\"node23\">\n<title>140364739332136</title>\n<polygon fill=\"none\" points=\"1139,-511.5 1139,-547.5 1303,-547.5 1303,-511.5 1139,-511.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1221\" y=\"-525.8\">feature_map: Concatenate</text>\n</g>\n<!-- 140364739336624&#45;&gt;140364739332136 -->\n<g class=\"edge\" id=\"edge14\">\n<title>140364739336624-&gt;140364739332136</title>\n<path d=\"M466.6259,-584.9809C469.7799,-584.6419 472.9083,-584.3142 476,-584 710.4353,-560.1729 987.3956,-542.7135 1128.4098,-534.5862\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1128.9552,-538.0607 1138.7382,-533.9936 1128.5541,-531.0722 1128.9552,-538.0607\" stroke=\"#000000\"/>\n</g>\n<!-- 140364739337072&#45;&gt;140364739332136 -->\n<g class=\"edge\" id=\"edge15\">\n<title>140364739337072-&gt;140364739332136</title>\n<path d=\"M765.3787,-585.0617C768.2777,-584.7044 771.1541,-584.3502 774,-584 896.6556,-568.9082 1038.2685,-551.6744 1128.6614,-540.6981\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1129.1023,-544.1703 1138.6075,-539.4905 1128.2585,-537.2214 1129.1023,-544.1703\" stroke=\"#000000\"/>\n</g>\n<!-- 140364739343080&#45;&gt;140364739332136 -->\n<g class=\"edge\" id=\"edge16\">\n<title>140364739343080-&gt;140364739332136</title>\n<path d=\"M996.6629,-584.4551C1039.3662,-573.9942 1093.2731,-560.7888 1137.5408,-549.9447\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1138.5302,-553.3059 1147.4103,-547.527 1136.8647,-546.5069 1138.5302,-553.3059\" stroke=\"#000000\"/>\n</g>\n<!-- 140364739311992&#45;&gt;140364739332136 -->\n<g class=\"edge\" id=\"edge17\">\n<title>140364739311992-&gt;140364739332136</title>\n<path d=\"M1221,-584.4551C1221,-576.3828 1221,-566.6764 1221,-557.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1224.5001,-557.5903 1221,-547.5904 1217.5001,-557.5904 1224.5001,-557.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140364739304640&#45;&gt;140364739332136 -->\n<g class=\"edge\" id=\"edge18\">\n<title>140364739304640-&gt;140364739332136</title>\n<path d=\"M1445.3371,-584.4551C1402.6338,-573.9942 1348.7269,-560.7888 1304.4592,-549.9447\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1305.1353,-546.5069 1294.5897,-547.527 1303.4698,-553.3059 1305.1353,-546.5069\" stroke=\"#000000\"/>\n</g>\n<!-- 140364738396848 -->\n<g class=\"node\" id=\"node19\">\n<title>140364738396848</title>\n<polygon fill=\"none\" points=\"1677,-584.5 1677,-620.5 1789,-620.5 1789,-584.5 1677,-584.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1733\" y=\"-598.8\">Age: InputLayer</text>\n</g>\n<!-- 140364738396848&#45;&gt;140364739332136 -->\n<g class=\"edge\" id=\"edge19\">\n<title>140364738396848-&gt;140364739332136</title>\n<path d=\"M1676.7617,-585.9134C1673.8086,-585.2245 1670.8752,-584.5807 1668,-584 1546.4964,-559.458 1404.0295,-544.4101 1313.1766,-536.5102\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1313.4433,-533.0204 1303.1805,-535.6518 1312.8443,-539.9947 1313.4433,-533.0204\" stroke=\"#000000\"/>\n</g>\n<!-- 140364738880120 -->\n<g class=\"node\" id=\"node20\">\n<title>140364738880120</title>\n<polygon fill=\"none\" points=\"1807,-584.5 1807,-620.5 1915,-620.5 1915,-584.5 1807,-584.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1861\" y=\"-598.8\">Fee: InputLayer</text>\n</g>\n<!-- 140364738880120&#45;&gt;140364739332136 -->\n<g class=\"edge\" id=\"edge20\">\n<title>140364738880120-&gt;140364739332136</title>\n<path d=\"M1806.8656,-585.8885C1803.8797,-585.1961 1800.9105,-584.5594 1798,-584 1628.9483,-551.5059 1428.1458,-538.1023 1313.5351,-532.7855\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1313.545,-529.2825 1303.3967,-532.3259 1313.2279,-536.2753 1313.545,-529.2825\" stroke=\"#000000\"/>\n</g>\n<!-- 140364738145864&#45;&gt;140364739332136 -->\n<g class=\"edge\" id=\"edge21\">\n<title>140364738145864-&gt;140364739332136</title>\n<path d=\"M183.6418,-585.1111C186.4567,-584.7114 189.2483,-584.3391 192,-584 371.8036,-561.8409 911.1777,-540.6315 1128.6285,-532.7389\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1128.8704,-536.2326 1138.7373,-532.3732 1128.6172,-529.2371 1128.8704,-536.2326\" stroke=\"#000000\"/>\n</g>\n<!-- 140364733512336 -->\n<g class=\"node\" id=\"node22\">\n<title>140364733512336</title>\n<polygon fill=\"none\" points=\"1933,-584.5 1933,-620.5 2103,-620.5 2103,-584.5 1933,-584.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2018\" y=\"-598.8\">direct_features: InputLayer</text>\n</g>\n<!-- 140364733512336&#45;&gt;140364739332136 -->\n<g class=\"edge\" id=\"edge22\">\n<title>140364733512336-&gt;140364739332136</title>\n<path d=\"M1932.7669,-585.274C1929.8127,-584.8209 1926.8839,-584.3942 1924,-584 1706.2028,-554.2294 1448.0535,-539.4082 1313.3261,-533.212\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1313.1991,-529.7027 1303.0505,-532.7452 1312.8813,-536.6955 1313.1991,-529.7027\" stroke=\"#000000\"/>\n</g>\n<!-- 140364739320016 -->\n<g class=\"node\" id=\"node24\">\n<title>140364739320016</title>\n<polygon fill=\"none\" points=\"1175,-438.5 1175,-474.5 1267,-474.5 1267,-438.5 1175,-438.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1221\" y=\"-452.8\">dense: Dense</text>\n</g>\n<!-- 140364739332136&#45;&gt;140364739320016 -->\n<g class=\"edge\" id=\"edge23\">\n<title>140364739332136-&gt;140364739320016</title>\n<path d=\"M1221,-511.4551C1221,-503.3828 1221,-493.6764 1221,-484.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1224.5001,-484.5903 1221,-474.5904 1217.5001,-484.5904 1224.5001,-484.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140364739320688 -->\n<g class=\"node\" id=\"node25\">\n<title>140364739320688</title>\n<polygon fill=\"none\" points=\"1093,-365.5 1093,-401.5 1349,-401.5 1349,-365.5 1093,-365.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1221\" y=\"-379.8\">batch_normalization: BatchNormalization</text>\n</g>\n<!-- 140364739320016&#45;&gt;140364739320688 -->\n<g class=\"edge\" id=\"edge24\">\n<title>140364739320016-&gt;140364739320688</title>\n<path d=\"M1221,-438.4551C1221,-430.3828 1221,-420.6764 1221,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1224.5001,-411.5903 1221,-401.5904 1217.5001,-411.5904 1224.5001,-411.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140364739327480 -->\n<g class=\"node\" id=\"node26\">\n<title>140364739327480</title>\n<polygon fill=\"none\" points=\"1161.5,-292.5 1161.5,-328.5 1280.5,-328.5 1280.5,-292.5 1161.5,-292.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1221\" y=\"-306.8\">dropout: Dropout</text>\n</g>\n<!-- 140364739320688&#45;&gt;140364739327480 -->\n<g class=\"edge\" id=\"edge25\">\n<title>140364739320688-&gt;140364739327480</title>\n<path d=\"M1221,-365.4551C1221,-357.3828 1221,-347.6764 1221,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1224.5001,-338.5903 1221,-328.5904 1217.5001,-338.5904 1224.5001,-338.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140364739513256 -->\n<g class=\"node\" id=\"node27\">\n<title>140364739513256</title>\n<polygon fill=\"none\" points=\"1167.5,-219.5 1167.5,-255.5 1274.5,-255.5 1274.5,-219.5 1167.5,-219.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1221\" y=\"-233.8\">dense_1: Dense</text>\n</g>\n<!-- 140364739327480&#45;&gt;140364739513256 -->\n<g class=\"edge\" id=\"edge26\">\n<title>140364739327480-&gt;140364739513256</title>\n<path d=\"M1221,-292.4551C1221,-284.3828 1221,-274.6764 1221,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1224.5001,-265.5903 1221,-255.5904 1217.5001,-265.5904 1224.5001,-265.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140364735160504 -->\n<g class=\"node\" id=\"node28\">\n<title>140364735160504</title>\n<polygon fill=\"none\" points=\"1085.5,-146.5 1085.5,-182.5 1356.5,-182.5 1356.5,-146.5 1085.5,-146.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1221\" y=\"-160.8\">batch_normalization_1: BatchNormalization</text>\n</g>\n<!-- 140364739513256&#45;&gt;140364735160504 -->\n<g class=\"edge\" id=\"edge27\">\n<title>140364739513256-&gt;140364735160504</title>\n<path d=\"M1221,-219.4551C1221,-211.3828 1221,-201.6764 1221,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1224.5001,-192.5903 1221,-182.5904 1217.5001,-192.5904 1224.5001,-192.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140364730779520 -->\n<g class=\"node\" id=\"node29\">\n<title>140364730779520</title>\n<polygon fill=\"none\" points=\"1154,-73.5 1154,-109.5 1288,-109.5 1288,-73.5 1154,-73.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1221\" y=\"-87.8\">dropout_1: Dropout</text>\n</g>\n<!-- 140364735160504&#45;&gt;140364730779520 -->\n<g class=\"edge\" id=\"edge28\">\n<title>140364735160504-&gt;140364730779520</title>\n<path d=\"M1221,-146.4551C1221,-138.3828 1221,-128.6764 1221,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1224.5001,-119.5903 1221,-109.5904 1217.5001,-119.5904 1224.5001,-119.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140364728647296 -->\n<g class=\"node\" id=\"node30\">\n<title>140364728647296</title>\n<polygon fill=\"none\" points=\"1172.5,-.5 1172.5,-36.5 1269.5,-36.5 1269.5,-.5 1172.5,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1221\" y=\"-14.8\">output: Dense</text>\n</g>\n<!-- 140364730779520&#45;&gt;140364728647296 -->\n<g class=\"edge\" id=\"edge29\">\n<title>140364730779520-&gt;140364728647296</title>\n<path d=\"M1221,-73.4551C1221,-65.3828 1221,-55.6764 1221,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1224.5001,-46.5903 1221,-36.5904 1217.5001,-46.5904 1224.5001,-46.5903\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"},"metadata":{"tags":[]},"execution_count":82}]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"PMFADoScQSkh","colab_type":"text"},"source":["## Entrenando el modelo\n","\n","Para entrenar el modelo es igual al caso anterior, ya generados el conjunto de datos correspondiente. Lo entrenamos con ayuda de `mlflow`."]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"nkbyR14JQSki","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"18947a84-91c0-4905-f39e-ba3fd6fb8fee","executionInfo":{"status":"ok","timestamp":1574907587498,"user_tz":180,"elapsed":397947,"user":{"displayName":"Claudio Sárate","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAyvJcBOeDmirReKch-IfaEhczbBFhZkXiSlm968g=s64","userId":"01992612555747043556"}}},"source":["import mlflow\n","\n","mlflow.set_experiment('awesome_advanced_approach')\n","\n","with mlflow.start_run(nested=True):\n","    # Log model hiperparameters first\n","    mlflow.log_param('description_features_layer_size', DESCRIPTION_FEATURES_LAYER_SIZE)\n","    mlflow.log_param('hidden_layer_size', HIDDEN_LAYER_SIZE)\n","    mlflow.log_param('embedded_columns', embedded_columns)\n","    mlflow.log_param('one_hot_columns', one_hot_columns)\n","    mlflow.log_param('numerical_columns', numeric_columns)\n","    \n","    # Train\n","    epochs = 30\n","    history = model.fit(train_dataset, epochs=epochs)\n","    \n","    # Evaluate\n","    loss, accuracy = model.evaluate(dev_dataset, verbose=0)\n","    print(\"\\n*** Validation loss: {} - accuracy: {}\".format(loss, accuracy))\n","    mlflow.log_metric('epochs', epochs)\n","    mlflow.log_metric('train_loss', history.history[\"loss\"][-1])\n","    mlflow.log_metric('train_accuracy', history.history[\"accuracy\"][-1])\n","    mlflow.log_metric('validation_loss', loss)\n","    mlflow.log_metric('validation_accuracy', accuracy)"],"execution_count":83,"outputs":[{"output_type":"stream","text":["Epoch 1/30\n","67/67 [==============================] - 15s 229ms/step - loss: 2.3182 - accuracy: 0.2170\n","Epoch 2/30\n","67/67 [==============================] - 12s 175ms/step - loss: 1.8948 - accuracy: 0.3133\n","Epoch 3/30\n","67/67 [==============================] - 12s 176ms/step - loss: 1.7193 - accuracy: 0.3836\n","Epoch 4/30\n","67/67 [==============================] - 12s 179ms/step - loss: 1.5512 - accuracy: 0.4584\n","Epoch 5/30\n","67/67 [==============================] - 12s 176ms/step - loss: 1.4091 - accuracy: 0.5155\n","Epoch 6/30\n","67/67 [==============================] - 12s 175ms/step - loss: 1.2473 - accuracy: 0.5995\n","Epoch 7/30\n","67/67 [==============================] - 12s 175ms/step - loss: 1.0744 - accuracy: 0.6767\n","Epoch 8/30\n","67/67 [==============================] - 12s 175ms/step - loss: 0.9340 - accuracy: 0.7407\n","Epoch 9/30\n","67/67 [==============================] - 12s 175ms/step - loss: 0.7798 - accuracy: 0.7934\n","Epoch 10/30\n","67/67 [==============================] - 12s 175ms/step - loss: 0.6949 - accuracy: 0.8221\n","Epoch 11/30\n","67/67 [==============================] - 12s 175ms/step - loss: 0.5757 - accuracy: 0.8654\n","Epoch 12/30\n","67/67 [==============================] - 12s 174ms/step - loss: 0.4772 - accuracy: 0.9005\n","Epoch 13/30\n","67/67 [==============================] - 12s 176ms/step - loss: 0.4598 - accuracy: 0.9054\n","Epoch 14/30\n","67/67 [==============================] - 12s 175ms/step - loss: 0.3766 - accuracy: 0.9316\n","Epoch 15/30\n","67/67 [==============================] - 12s 174ms/step - loss: 0.3512 - accuracy: 0.9381\n","Epoch 16/30\n","67/67 [==============================] - 12s 175ms/step - loss: 0.3282 - accuracy: 0.9399\n","Epoch 17/30\n","67/67 [==============================] - 12s 174ms/step - loss: 0.2897 - accuracy: 0.9543\n","Epoch 18/30\n","67/67 [==============================] - 12s 173ms/step - loss: 0.2836 - accuracy: 0.9530\n","Epoch 19/30\n","67/67 [==============================] - 12s 176ms/step - loss: 0.2644 - accuracy: 0.9538\n","Epoch 20/30\n","67/67 [==============================] - 12s 179ms/step - loss: 0.3446 - accuracy: 0.9270\n","Epoch 21/30\n","67/67 [==============================] - 12s 174ms/step - loss: 0.2713 - accuracy: 0.9493\n","Epoch 22/30\n","67/67 [==============================] - 12s 175ms/step - loss: 0.3022 - accuracy: 0.9357\n","Epoch 23/30\n","67/67 [==============================] - 12s 173ms/step - loss: 0.3871 - accuracy: 0.9125\n","Epoch 24/30\n","67/67 [==============================] - 12s 174ms/step - loss: 0.2747 - accuracy: 0.9453\n","Epoch 25/30\n","67/67 [==============================] - 12s 175ms/step - loss: 0.2511 - accuracy: 0.9523\n","Epoch 26/30\n","67/67 [==============================] - 12s 174ms/step - loss: 0.2277 - accuracy: 0.9552\n","Epoch 27/30\n","67/67 [==============================] - 12s 174ms/step - loss: 0.2148 - accuracy: 0.9596\n","Epoch 28/30\n","67/67 [==============================] - 12s 172ms/step - loss: 0.2449 - accuracy: 0.9512\n","Epoch 29/30\n","67/67 [==============================] - 12s 174ms/step - loss: 0.2520 - accuracy: 0.9466\n","Epoch 30/30\n","67/67 [==============================] - 12s 177ms/step - loss: 0.2059 - accuracy: 0.9581\n","\n","*** Validation loss: 0.21898142642834606 - accuracy: 0.9555975198745728\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"4hCoULwUQSkl","colab_type":"text"},"source":["## Evaluando el modelo sobre los datos de evaluación para la competencia\n","\n","Una vez que tenemos definido nuestro modelo, el último paso es ponerlo a prueba en los datos de evaluación para generar un archivo para enviar a la competencia Kaggle.\n","\n","Comenzamos cargando el conjunto de datos."]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"6zp-oOlEQSkm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":484},"outputId":"1c8f0c78-4604-4178-faf8-3aa87b09870b","executionInfo":{"status":"ok","timestamp":1574907587906,"user_tz":180,"elapsed":398344,"user":{"displayName":"Claudio Sárate","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAyvJcBOeDmirReKch-IfaEhczbBFhZkXiSlm968g=s64","userId":"01992612555747043556"}}},"source":["test_dataset = pd.read_csv(os.path.join(BASE_DIR, 'test.csv'))\n","test_dataset.head()"],"execution_count":84,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Type</th>\n","      <th>Age</th>\n","      <th>Breed1</th>\n","      <th>Breed2</th>\n","      <th>Gender</th>\n","      <th>Color1</th>\n","      <th>Color2</th>\n","      <th>Color3</th>\n","      <th>MaturitySize</th>\n","      <th>FurLength</th>\n","      <th>Vaccinated</th>\n","      <th>Dewormed</th>\n","      <th>Sterilized</th>\n","      <th>Health</th>\n","      <th>Quantity</th>\n","      <th>Fee</th>\n","      <th>State</th>\n","      <th>Description</th>\n","      <th>PID</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>265</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>41401</td>\n","      <td>I just found it alone yesterday near my apartm...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>307</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>41326</td>\n","      <td>Their pregnant mother was dumped by her irresp...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>307</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>7</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>41326</td>\n","      <td>Siu Pak just give birth on 13/6/10 to 6puppies...</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2</td>\n","      <td>12</td>\n","      <td>265</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>41326</td>\n","      <td>Very manja and gentle stray cat found, we woul...</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>264</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>50</td>\n","      <td>41326</td>\n","      <td>Kali is a super playful kitten who is on the g...</td>\n","      <td>11</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Type  Age  ...                                        Description  PID\n","0     2    1  ...  I just found it alone yesterday near my apartm...    1\n","1     1    1  ...  Their pregnant mother was dumped by her irresp...    2\n","2     1    0  ...  Siu Pak just give birth on 13/6/10 to 6puppies...    7\n","3     2   12  ...  Very manja and gentle stray cat found, we woul...    9\n","4     2    3  ...  Kali is a super playful kitten who is on the g...   11\n","\n","[5 rows x 19 columns]"]},"metadata":{"tags":[]},"execution_count":84}]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"ZKxz4HX6QSkp","colab_type":"text"},"source":["## Creamos el conjunto de datos para darle al modelo entrenado"]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"pPkPPgz3QSkq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":538},"outputId":"7a8c7fec-1e9a-45af-bc08-037f1ad020cf","executionInfo":{"status":"ok","timestamp":1574907591320,"user_tz":180,"elapsed":401751,"user":{"displayName":"Claudio Sárate","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAyvJcBOeDmirReKch-IfaEhczbBFhZkXiSlm968g=s64","userId":"01992612555747043556"}}},"source":["# First tokenize the description\n","\n","test_dataset[\"TokenizedDescription\"] = test_dataset[\"Description\"]\\\n","    .fillna(value=\"\").apply(tokenize_description)\n","\n","# Generate the basic TF dataset\n","\n","tf_test_dataset = tf.data.Dataset.from_generator(\n","    lambda: dataset_generator(test_dataset, True),\n","    output_types=instance_types  # It should have the same instance types\n",")\n","\n","for data in tf_test_dataset.take(2):  # The dataset only returns a data instance now (no target)\n","    pprint(data)\n","    print()"],"execution_count":85,"outputs":[{"output_type":"stream","text":["{'Age': <tf.Tensor: id=18759, shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n"," 'Breed1': <tf.Tensor: id=18760, shape=(1,), dtype=int32, numpy=array([265], dtype=int32)>,\n"," 'Breed2': <tf.Tensor: id=18761, shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n"," 'Color1': <tf.Tensor: id=18762, shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n"," 'Color2': <tf.Tensor: id=18763, shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>,\n"," 'Color3': <tf.Tensor: id=18764, shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n"," 'Fee': <tf.Tensor: id=18765, shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n"," 'description': <tf.Tensor: id=18766, shape=(13,), dtype=int32, numpy=\n","array([ 116,  429, 1371,  991,  189,    1, 7873, 1043,   62,  600,  728,\n","          5,    1], dtype=int32)>,\n"," 'direct_features': <tf.Tensor: id=18767, shape=(24,), dtype=float32, numpy=\n","array([0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n","       1., 0., 0., 1., 1., 0., 0.], dtype=float32)>}\n","\n","{'Age': <tf.Tensor: id=18768, shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n"," 'Breed1': <tf.Tensor: id=18769, shape=(1,), dtype=int32, numpy=array([307], dtype=int32)>,\n"," 'Breed2': <tf.Tensor: id=18770, shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n"," 'Color1': <tf.Tensor: id=18771, shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>,\n"," 'Color2': <tf.Tensor: id=18772, shape=(1,), dtype=int32, numpy=array([7], dtype=int32)>,\n"," 'Color3': <tf.Tensor: id=18773, shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n"," 'Fee': <tf.Tensor: id=18774, shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n"," 'description': <tf.Tensor: id=18775, shape=(47,), dtype=int32, numpy=\n","array([ 945,  154,  256, 2049,  105,  403,  991, 4677,  552,  545,    1,\n","        142,  134,  403,    1,  118,  210,   73,    1,  533,  387,   35,\n","        394,  272,   98,   62,    1,  464,  411,  151, 1401,   42,  253,\n","          1,  825,   35, 4659,  247, 4155, 1402, 1403,    1,   43,   52,\n","        599,   38,    1], dtype=int32)>,\n"," 'direct_features': <tf.Tensor: id=18776, shape=(24,), dtype=float32, numpy=\n","array([1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0.,\n","       0., 0., 1., 0., 1., 0., 0.], dtype=float32)>}\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"JQ5uVG-JQSks","colab_type":"text"},"source":["## Padding batches"]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"9fqTuEH5QSkt","colab_type":"code","colab":{}},"source":["test_data = tf_test_dataset.padded_batch(\n","    BATCH_SIZE, \n","    padded_shapes=padding_shapes[0], \n","    padding_values=padding_values[0]\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"KEVmqQ6EQSkw","colab_type":"text"},"source":["## Correr el modelo\n","\n","El último paso es correr el modelo sobre los datos de evaluación para conseguir las predicciones a enviar a la competencia."]},{"cell_type":"code","metadata":{"Collapsed":"false","id":"nSeSqhOWQSkw","colab_type":"code","colab":{}},"source":["test_dataset[\"AdoptionSpeed\"] = model.predict(test_data).argmax(axis=1)\n","\n","test_dataset.to_csv(BASE_DIR+\"submission.csv\", index=False, columns=[\"PID\", \"AdoptionSpeed\"])"],"execution_count":0,"outputs":[]}]}