{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\"\"\"Exercise 1\n",
    "\n",
    "Usage:\n",
    "\n",
    "$ CUDA_VISIBLE_DEVICES=2 python practico_1_train_petfinder.py --dataset_dir ../ --epochs 30 --dropout 0.1 0.1 --hidden_layer_sizes 200 100\n",
    "\n",
    "To know which GPU to use, you can check it with the command\n",
    "\n",
    "$ nvidia-smi\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### PrÃ¡ctico 1 Aprendizaje Profundo de: Claudio Sarate - Martin Hunziker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/claudio/anaconda3/envs/MachineLearning/lib/python3.7/site-packages/botocore/vendored/requests/packages/urllib3/_collections.py:1: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Mapping, MutableMapping\n",
      "/home/claudio/anaconda3/envs/MachineLearning/lib/python3.7/site-packages/ipykernel_launcher.py:13: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import mlflow\n",
    "import numpy\n",
    "import pandas\n",
    "import easydict\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from itertools import zip_longest\n",
    "from collections import Iterable, Iterator\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "in_colab = 'google.colab' in sys.modules\n",
    "if in_colab:\n",
    "    !pip install tensorflow==2.0.0\n",
    "    !pip install mlflow\n",
    "    print(tf.__version__)\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    BASE_DIR = \"/content/drive/My Drive/Colab Notebooks/Machine Learning/Aprendizaje Profundo/Data/\"\n",
    "else:\n",
    "    BASE_DIR = \"./Data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "TARGET_COL = 'AdoptionSpeed'\n",
    "embed_size = 5"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "os.environ.keys()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "'jupyter_notebook' in sys.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def read_args():\n",
    "   \n",
    "    #parser = argparse.ArgumentParser(\n",
    "\n",
    "#    description='Training a MLP on the petfinder dataset')\n",
    "    # Here you have some examples of classifier parameters. You can add\n",
    "    # more arguments or change these if you need to.\n",
    "#    parser.add_argument('--dataset_dir', default=BASE_DIR, type=str,\n",
    "#                        help='Directory with the training and test files.')\n",
    "#    parser.add_argument('--hidden_layer_sizes', nargs='+', default=[100], type=int,\n",
    "#                        help='Number of hidden units of each hidden layer.')\n",
    "#    parser.add_argument('--epochs', default=10, type=int,\n",
    "#                        help='Number of epochs to train.')\n",
    "#    parser.add_argument('--dropout', nargs='+', default=[0.5], type=float,\n",
    "#                        help='Dropout ratio for every layer.')\n",
    "#    parser.add_argument('--batch_size', type=int, default=64,\n",
    "#                        help='Number of instances in each batch.')\n",
    "#    parser.add_argument('--one_hot_cols', type=str, nargs='+', default=['Gender', 'Color1'],\n",
    "#                        help='One hot columns.')\n",
    "#    parser.add_argument('--numeric_cols', type=str, nargs='+', default=['Age', 'Fee'],\n",
    "#                        help='Numeric columns.')\n",
    "#    parser.add_argument('--embedded_cols', type=str, nargs='+', default=['Breed1'],\n",
    "#                        help='Embedded columns.')\n",
    "#    parser.add_argument('--experiment_name', type=str, default='Base model',\n",
    "#                        help='Name of the experiment, used in mlflow.')\n",
    "#    parser.add_argument('--run_name', type=str, default='run00',\n",
    "#                        help='Name of the run, used in mlflow.')\n",
    "#    args = parser.parse_args()\n",
    "\n",
    "#    assert len(args.hidden_layer_sizes) == len(args.dropout)\n",
    "#    return args\n",
    "\n",
    "    #Cambio para correr en jupyter notebook con argumentos que se definen dentro\n",
    "    args = easydict.EasyDict({'dataset_dir': BASE_DIR,\n",
    "                              'hidden_layer_sizes': [128, 64, 32],\n",
    "                              'epochs': 30,\n",
    "                              'dropout': [0.35, 0.40, 0.45],\n",
    "                              'batch_size': 64,\n",
    "                              'one_hot_cols': ['Gender', 'Color1', 'MaturitySize', \n",
    "                                               'Vaccinated', 'Dewormed','Health'],\n",
    "                              'numeric_cols': ['Age', 'Fee'],\n",
    "                              'embedded_cols': ['Breed1'],\n",
    "                              'experiment_name': 'Base Model',\n",
    "                              'run_name': 'run04'})\n",
    "\n",
    "    assert len(args.hidden_layer_sizes) == len(args.dropout)\n",
    "    print (args)\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def process_features(df, one_hot_columns, numeric_columns, embedded_columns, test=False):\n",
    "    direct_features = []\n",
    "\n",
    "    # Create one hot encodings\n",
    "    for one_hot_col, max_value in one_hot_columns.items():\n",
    "        direct_features.append(tf.keras.utils.to_categorical(df[one_hot_col] - 1, max_value))\n",
    "\n",
    "    # TODO Create and append numeric columns\n",
    "    for column in numeric_columns:\n",
    "        direct_features.append(tf.keras.utils.normalize(df[column].values.reshape(-1,1)))\n",
    "\n",
    "#    Otras Alternativas\n",
    "\n",
    "#        scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "#        scaler.fit(df[column].values.reshape(-1, 1))\n",
    "#        df[column] = scaler.transform(df[column].values.reshape(-1,1)).reshape(-1)\n",
    "#    for col in df[numeric_columns]:\n",
    "#        direct_features.append(df[numeric_columns].values)   \n",
    "\n",
    "#    Otra alternativa\n",
    "#        scaler_features = preprocessing.StandardScaler().fit_transform(df[[num_column]].values)\n",
    "#        direct_features.append(scaler_features)\n",
    "\n",
    "    # Concatenate all features that don't need further embedding into a single matrix.\n",
    "  \n",
    "    features = {'direct_features': numpy.hstack(direct_features)}\n",
    "\n",
    "    # Create embedding columns - nothing to do here. We will use the zero embedding for OOV\n",
    "    for embedded_col in embedded_columns.keys():\n",
    "        features[embedded_col] = df[embedded_col].values\n",
    "\n",
    "    if not test:\n",
    "        nlabels = df[TARGET_COL].unique().shape[0]\n",
    "        # Convert labels to one-hot encodings\n",
    "        targets = tf.keras.utils.to_categorical(df[TARGET_COL], nlabels)\n",
    "    else:\n",
    "        targets = None\n",
    "    \n",
    "    return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def load_dataset(dataset_dir, batch_size):\n",
    "\n",
    "    # Read train dataset (and maybe dev, if you need to...)\n",
    "    dataset, dev_dataset = train_test_split(\n",
    "        pandas.read_csv(os.path.join(dataset_dir, 'train.csv')), test_size=0.2)\n",
    "    \n",
    "    test_dataset = pandas.read_csv(os.path.join(dataset_dir, 'test.csv'))\n",
    "    \n",
    "    print('Training samples {}, test_samples {}'.format(\n",
    "        dataset.shape[0], test_dataset.shape[0]))\n",
    "    \n",
    "    return dataset, dev_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def define_model(embedded_input, direct_input, hidden_layers, dropouts, nlabels):\n",
    "    tf.keras.backend.clear_session()\n",
    "    initializer = tf.keras.initializers.glorot_normal(seed=1234)\n",
    "\n",
    "\n",
    "    # Add one input and one embedding for each embedded column\n",
    "    embedding_layers = []\n",
    "    inputs           = []\n",
    "    for embedded_col, max_value in embedded_input.items():\n",
    "        input_layer = layers.Input(shape=(1,), name=embedded_col)\n",
    "        inputs.append(input_layer)\n",
    "        # Define the embedding layer\n",
    "        embedding_size = int(max_value / embed_size)\n",
    "        embedding_layers.append(\n",
    "            tf.squeeze(layers.Embedding(input_dim=max_value, output_dim=embedding_size)(input_layer), axis=-2))\n",
    "        print('Adding embedding of size {} for layer {}'.format(embedding_size, embedded_col))\n",
    "\n",
    "    # Add the direct features already calculated\n",
    "    direct_features_input_shape = direct_input.shape[1]\n",
    "    direct_features_input = layers.Input(shape=direct_features_input_shape, name='direct_features')\n",
    "    inputs.append(direct_features_input)\n",
    "\n",
    "    # Concatenate everything together\n",
    "    features = layers.concatenate(embedding_layers + [direct_features_input])\n",
    "    \n",
    "    # Creating Models\n",
    "    n_layers = len(hidden_layers)\n",
    "    if len(dropouts) > n_layers:\n",
    "        dropouts = dropouts[:n_layers]\n",
    "        \n",
    "    for n_neurons, drop, layer in zip_longest(hidden_layers, dropouts, range(n_layers)):\n",
    "        if layer == 0:\n",
    "            dense      = layers.Dense(n_neurons, activation='relu', kernel_initializer=initializer)(features)\n",
    "            last_layer = dense\n",
    "        else:\n",
    "            dense      = layers.Dense(n_neurons, activation='relu', kernel_initializer=initializer)(last_layer)\n",
    "            last_layer = dense\n",
    "        if drop is not None:\n",
    "            #drop_layer = layers.BatchNormalization()(last_layer)\n",
    "            #last_layer = drop_layer\n",
    "            drop_layer = layers.Dropout(drop)(last_layer)\n",
    "            last_layer = drop_layer\n",
    "    \n",
    "    output_layer = layers.Dense(nlabels, activation='softmax')(last_layer)\n",
    "    model        = models.Model(inputs=inputs, outputs=output_layer)            \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_dir': './Data/', 'hidden_layer_sizes': [128, 64, 32], 'epochs': 30, 'dropout': [0.35, 0.4, 0.45], 'batch_size': 64, 'one_hot_cols': ['Gender', 'Color1', 'MaturitySize', 'Vaccinated', 'Dewormed', 'Health'], 'numeric_cols': ['Age', 'Fee'], 'embedded_cols': ['Breed1'], 'experiment_name': 'Base Model', 'run_name': 'run04'}\n",
      "Training samples 8465, test_samples 4411\n",
      "Adding embedding of size 61 for layer Breed1\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Breed1 (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 61)        18788       Breed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Squeeze (TensorFlow [(None, 61)]         0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "direct_features (InputLayer)    [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 86)           0           tf_op_layer_Squeeze[0][0]        \n",
      "                                                                 direct_features[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          11136       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 128)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           8256        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           2080        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 5)            165         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 40,425\n",
      "Trainable params: 40,425\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Running:  run04\n",
      "Epoch 1/30\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 1.5273 - accuracy: 0.2697 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/30\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.4857 - accuracy: 0.2809 - val_loss: 1.4456 - val_accuracy: 0.3245\n",
      "Epoch 3/30\n",
      "133/133 [==============================] - 1s 5ms/step - loss: 1.4707 - accuracy: 0.2878 - val_loss: 1.4339 - val_accuracy: 0.3335\n",
      "Epoch 4/30\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.4598 - accuracy: 0.3142 - val_loss: 1.4325 - val_accuracy: 0.3340\n",
      "Epoch 5/30\n",
      "133/133 [==============================] - 1s 4ms/step - loss: 1.4561 - accuracy: 0.3125 - val_loss: 1.4254 - val_accuracy: 0.3321\n",
      "Epoch 6/30\n",
      "133/133 [==============================] - 1s 4ms/step - loss: 1.4454 - accuracy: 0.3229 - val_loss: 1.4252 - val_accuracy: 0.3316\n",
      "Epoch 7/30\n",
      "133/133 [==============================] - 1s 5ms/step - loss: 1.4422 - accuracy: 0.3268 - val_loss: 1.4248 - val_accuracy: 0.3363\n",
      "Epoch 8/30\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4394 - accuracy: 0.3258 - val_loss: 1.4231 - val_accuracy: 0.3354\n",
      "Epoch 9/30\n",
      "133/133 [==============================] - 1s 4ms/step - loss: 1.4328 - accuracy: 0.3295 - val_loss: 1.4204 - val_accuracy: 0.3444\n",
      "Epoch 10/30\n",
      "133/133 [==============================] - 1s 4ms/step - loss: 1.4322 - accuracy: 0.3327 - val_loss: 1.4208 - val_accuracy: 0.3349\n",
      "Epoch 11/30\n",
      "133/133 [==============================] - 1s 4ms/step - loss: 1.4272 - accuracy: 0.3350 - val_loss: 1.4213 - val_accuracy: 0.3354\n",
      "Epoch 12/30\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4263 - accuracy: 0.3411 - val_loss: 1.4219 - val_accuracy: 0.3359\n",
      "Epoch 13/30\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4186 - accuracy: 0.3485 - val_loss: 1.4198 - val_accuracy: 0.3311\n",
      "Epoch 14/30\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4180 - accuracy: 0.3485 - val_loss: 1.4251 - val_accuracy: 0.3325\n",
      "Epoch 15/30\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4135 - accuracy: 0.3516 - val_loss: 1.4244 - val_accuracy: 0.3439\n",
      "Epoch 16/30\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4125 - accuracy: 0.3424 - val_loss: 1.4266 - val_accuracy: 0.3429\n",
      "Epoch 17/30\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4055 - accuracy: 0.3513 - val_loss: 1.4268 - val_accuracy: 0.3373\n",
      "Epoch 18/30\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.4044 - accuracy: 0.3519 - val_loss: 1.4343 - val_accuracy: 0.3387\n",
      "Epoch 19/30\n",
      "133/133 [==============================] - 1s 4ms/step - loss: 1.3992 - accuracy: 0.3610 - val_loss: 1.4367 - val_accuracy: 0.3434\n",
      "Epoch 20/30\n",
      "133/133 [==============================] - 1s 4ms/step - loss: 1.3962 - accuracy: 0.3569 - val_loss: 1.4353 - val_accuracy: 0.3477\n",
      "Epoch 21/30\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.3942 - accuracy: 0.3588 - val_loss: 1.4394 - val_accuracy: 0.3368\n",
      "Epoch 22/30\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.3871 - accuracy: 0.3677 - val_loss: 1.4462 - val_accuracy: 0.3415\n",
      "Epoch 23/30\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.3845 - accuracy: 0.3564 - val_loss: 1.4463 - val_accuracy: 0.3415\n",
      "Epoch 24/30\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.3858 - accuracy: 0.3579 - val_loss: 1.4530 - val_accuracy: 0.3401\n",
      "Epoch 25/30\n",
      "133/133 [==============================] - 0s 3ms/step - loss: 1.3793 - accuracy: 0.3660 - val_loss: 1.4549 - val_accuracy: 0.3425\n",
      "Epoch 26/30\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.3754 - accuracy: 0.3667 - val_loss: 1.4606 - val_accuracy: 0.3354\n",
      "Epoch 27/30\n",
      "133/133 [==============================] - 0s 4ms/step - loss: 1.3731 - accuracy: 0.3635 - val_loss: 1.4587 - val_accuracy: 0.3387\n",
      "Epoch 28/30\n",
      "133/133 [==============================] - 1s 4ms/step - loss: 1.3762 - accuracy: 0.3742 - val_loss: 1.4607 - val_accuracy: 0.3429\n",
      "Epoch 29/30\n",
      "133/133 [==============================] - 1s 4ms/step - loss: 1.3732 - accuracy: 0.3673 - val_loss: 1.4669 - val_accuracy: 0.3410\n",
      "Epoch 30/30\n",
      "133/133 [==============================] - 1s 4ms/step - loss: 1.3708 - accuracy: 0.3676 - val_loss: 1.4765 - val_accuracy: 0.3462\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 1.4765 - accuracy: 0.3462\n",
      "*** Dev loss: 1.476510433589711 - accuracy: 0.3462446928024292\n",
      "Predictions\n",
      "All operations completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/claudio/anaconda3/envs/MachineLearning/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py:364: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  if isinstance(inputs, collections.Sequence):\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    args = read_args()\n",
    "    dataset, dev_dataset, test_dataset = load_dataset(args.dataset_dir, args.batch_size)\n",
    "    nlabels = dataset[TARGET_COL].unique().shape[0]\n",
    "    \n",
    "    # It's important to always use the same one-hot length\n",
    "    one_hot_columns = {\n",
    "        one_hot_col: dataset[one_hot_col].max()\n",
    "        for one_hot_col in args.one_hot_cols\n",
    "    }\n",
    "    embedded_columns = {\n",
    "        embedded_col: dataset[embedded_col].max() + 1\n",
    "        for embedded_col in args.embedded_cols\n",
    "    }\n",
    "    numeric_columns = args.numeric_cols\n",
    "    \n",
    "    # TODO shuffle the train dataset! (ready)\n",
    "    from sklearn.utils import shuffle\n",
    "    dataset = shuffle(dataset, random_state=22)\n",
    "\n",
    "    # TODO (optional) put these three types of columns in the same dictionary with \"column types\" (ready)\n",
    "    X_train, y_train = process_features(dataset, one_hot_columns, numeric_columns, embedded_columns)\n",
    "    direct_features_input_shape = (X_train['direct_features'].shape[1],)\n",
    "    X_dev, y_dev = process_features(dev_dataset, one_hot_columns, numeric_columns, embedded_columns)\n",
    "    \n",
    "    # Create the tensorflow Dataset\n",
    "    batch_size = 64\n",
    "    \n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size)\n",
    "    dev_ds   = tf.data.Dataset.from_tensor_slices((X_dev, y_dev)).batch(batch_size)\n",
    "    test_ds  = tf.data.Dataset.from_tensor_slices(process_features(\n",
    "        test_dataset, one_hot_columns, numeric_columns, embedded_columns, test=True)[0]).batch(batch_size)\n",
    "\n",
    "    # TODO: Build the Keras model (Ready)\n",
    "    model = define_model(embedded_columns, X_train['direct_features'], args.hidden_layer_sizes,\n",
    "                               args.dropout, nlabels)\n",
    "        \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Weight classes\n",
    "    from sklearn.utils import class_weight\n",
    "    import numpy as np\n",
    "    class_weight = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(dataset.AdoptionSpeed.values),\n",
    "                                                 dataset.AdoptionSpeed.values)\n",
    "    \n",
    "    # TODO: Fit the model (Ready)\n",
    "    mlflow.set_experiment(args.experiment_name)\n",
    "    print(\"Running: \", args.run_name)\n",
    "    with mlflow.start_run(nested=True, run_name=args.run_name):\n",
    "        # Log model hiperparameters first\n",
    "        mlflow.log_param('hidden_layer_size', args.hidden_layer_sizes)\n",
    "        mlflow.log_param('dropout', args.dropout)\n",
    "        mlflow.log_param('embedded_columns', embedded_columns)\n",
    "        mlflow.log_param('one_hot_columns', one_hot_columns)\n",
    "        mlflow.log_param('numerical_columns', numeric_columns)\n",
    "        mlflow.log_param('total_columns', numeric_columns + list(one_hot_columns.keys()) + list(embedded_columns.keys()))\n",
    "        mlflow.log_param('epochs', args.epochs)\n",
    "        mlflow.log_param('n_params', model.count_params())\n",
    "\n",
    "        # Train\n",
    "        history = model.fit(train_ds, epochs=args.epochs,\n",
    "                            validation_data=dev_ds, verbose=1, class_weight=class_weight)\n",
    "                            \n",
    "\n",
    "        # TODO: analyze history to see if model converges/overfits\n",
    "        \n",
    "        # TODO: Evaluate the model, calculating the metrics.\n",
    "        # Option 1: Use the model.evaluate() method. For this, the model must be\n",
    "        # already compiled with the metrics.\n",
    "        #performance = model.evaluate(X_test, y_test) #(Ready)\n",
    "\n",
    "        #loss, accuracy = 0, 0\n",
    "        loss, accuracy = model.evaluate(dev_ds)\n",
    "        print(\"*** Dev loss: {} - accuracy: {}\".format(loss, accuracy))\n",
    "        mlflow.log_metric('dev_loss', loss)\n",
    "        mlflow.log_metric('dev_accuracy', accuracy)\n",
    "        for epoch in range(args.epochs):\n",
    "            mlflow.log_metric('hist_train_accuracy', value=history.history['accuracy'][epoch], step=epoch)\n",
    "            mlflow.log_metric('hist_val_accuracy', value=history.history['val_accuracy'][epoch], step=epoch)\n",
    "            mlflow.log_metric('hist_train_loss', value=history.history['loss'][epoch], step=epoch)\n",
    "            mlflow.log_metric('hist_val_loss', value=history.history['val_loss'][epoch], step=epoch)\n",
    "\n",
    "        # Option 2: Use the model.predict() method and calculate the metrics using\n",
    "        # sklearn. We recommend this, because you can store the predictions if\n",
    "        # you need more analysis later. Also, if you calculate the metrics on a\n",
    "        # notebook, then you can compare multiple classifiers.\n",
    "        \n",
    "        #predictions = 'No prediction yet'\n",
    "        print(\"Predictions\")\n",
    "        predictions = model.predict(test_ds)\n",
    "        \n",
    "        # TODO: Convert predictions to classes\n",
    "        # TODO: Save the results for submission\n",
    "        # ...\n",
    "        #print(collections.Counter(predictions))\n",
    "        test_dataset[\"AdoptionSpeed\"] = predictions.argmax(axis=1)\n",
    "        test_dataset.to_csv(\"./submission.csv\", index=False, columns=[\"PID\", \"AdoptionSpeed\"])\n",
    "        \n",
    "    print('All operations completed')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
